--- /tmp/zen-upstream-analysis/config.py	2025-10-06 13:16:45
+++ config.py	2025-08-18 16:59:55
@@ -1,36 +1,38 @@
 """
-Configuration and constants for Zen MCP Server
+Configuration and constants for HestAI MCP Server

-This module centralizes all configuration settings for the Zen MCP Server.
+This module centralizes all configuration settings for the HestAI MCP Server.
 It defines model configurations, token limits, temperature defaults, and other
 constants used throughout the application.

 Configuration values can be overridden by environment variables where appropriate.
 """

-from utils.env import get_env
+import os

 # Version and metadata
 # These values are used in server responses and for tracking releases
 # IMPORTANT: This is the single source of truth for version and author info
 # Semantic versioning: MAJOR.MINOR.PATCH
-__version__ = "7.4.0"
+__version__ = "5.8.2-premium"
 # Last update date in ISO format
-__updated__ = "2025-10-06"
+__updated__ = "2025-08-06"
 # Primary maintainer
 __author__ = "Fahad Gilani"
+# Server variant
+__variant__ = "Premium High-End Models Only"

 # Model configuration
 # DEFAULT_MODEL: The default model used for all AI operations
 # This should be a stable, high-performance model suitable for code analysis
 # Can be overridden by setting DEFAULT_MODEL environment variable
 # Special value "auto" means Claude should pick the best model for each task
-DEFAULT_MODEL = get_env("DEFAULT_MODEL", "auto") or "auto"
+DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "auto")

 # Auto mode detection - when DEFAULT_MODEL is "auto", Claude picks the model
 IS_AUTO_MODE = DEFAULT_MODEL.lower() == "auto"

-# Each provider (gemini.py, openai_provider.py, xai.py) defines its own MODEL_CAPABILITIES
+# Each provider (gemini.py, openai_provider.py, xai.py) defines its own SUPPORTED_MODELS
 # with detailed descriptions. Tools use ModelProviderRegistry.get_available_model_names()
 # to get models only from enabled providers (those with valid API keys).
 #
@@ -61,7 +63,7 @@
 # Thinking Mode Defaults
 # DEFAULT_THINKING_MODE_THINKDEEP: Default thinking depth for extended reasoning tool
 # Higher modes use more computational budget but provide deeper analysis
-DEFAULT_THINKING_MODE_THINKDEEP = get_env("DEFAULT_THINKING_MODE_THINKDEEP", "high") or "high"
+DEFAULT_THINKING_MODE_THINKDEEP = os.getenv("DEFAULT_THINKING_MODE_THINKDEEP", "high")

 # Consensus Tool Defaults
 # Consensus timeout and rate limiting settings
@@ -75,10 +77,10 @@
 #
 # IMPORTANT: This limit ONLY applies to the Claude CLI ↔ MCP Server transport boundary.
 # It does NOT limit internal MCP Server operations like system prompts, file embeddings,
-# conversation history, or content sent to external models (Gemini/OpenAI/OpenRouter).
+# conversation history, or content sent to external models (Gemini/O3/OpenRouter).
 #
 # MCP Protocol Architecture:
-# Claude CLI ←→ MCP Server ←→ External Model (Gemini/OpenAI/etc.)
+# Claude CLI ←→ MCP Server ←→ External Model (Gemini/O3/etc.)
 #     ↑                              ↑
 #     │                              │
 # MCP transport                Internal processing
@@ -117,7 +119,7 @@
         Maximum character count for user input prompts
     """
     # Check for Claude's MAX_MCP_OUTPUT_TOKENS environment variable
-    max_tokens_str = get_env("MAX_MCP_OUTPUT_TOKENS")
+    max_tokens_str = os.getenv("MAX_MCP_OUTPUT_TOKENS")

     if max_tokens_str:
         try:
@@ -143,7 +145,7 @@
 # Examples: "fr-FR", "en-US", "zh-CN", "zh-TW", "ja-JP", "ko-KR", "es-ES",
 # "de-DE", "it-IT", "pt-PT"
 # Leave empty for default language (English)
-LOCALE = get_env("LOCALE", "") or ""
+LOCALE = os.getenv("LOCALE", "")

 # Threading configuration
 # Simple in-memory conversation threading for stateless MCP environment
