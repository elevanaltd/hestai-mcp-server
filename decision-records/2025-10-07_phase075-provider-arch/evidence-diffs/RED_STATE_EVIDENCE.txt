============================= test session starts ==============================
platform darwin -- Python 3.13.7, pytest-8.4.1, pluggy-1.6.0 -- /opt/homebrew/opt/python@3.13/bin/python3.13
cachedir: .pytest_cache
rootdir: /Volumes/HestAI-Tools/hestai-mcp-server
configfile: pytest.ini
plugins: anyio-4.9.0, asyncio-1.1.0, cov-6.2.1
asyncio: mode=Mode.AUTO, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 6 items

tests/test_provider_shared.py::test_providertype_import FAILED           [ 16%]
tests/test_provider_shared.py::test_providertype_values FAILED           [ 33%]
tests/test_model_restrictions.py::TestAliasResolution::test_anthropic_provider_type_exists FAILED [ 50%]
tests/test_gemini_provider.py::test_gemini_provider_imports FAILED       [ 66%]
tests/test_gemini_provider.py::test_gemini_provider_no_deprecated_configure FAILED [ 83%]
tests/test_gemini_provider.py::test_gemini_provider_uses_client_pattern FAILED [100%]

=================================== FAILURES ===================================
___________________________ test_providertype_import ___________________________
tests/test_provider_shared.py:22: in test_providertype_import
    from providers.shared import ProviderType
E   ModuleNotFoundError: No module named 'providers.shared'
___________________________ test_providertype_values ___________________________
tests/test_provider_shared.py:36: in test_providertype_values
    from providers.shared import ProviderType
E   ModuleNotFoundError: No module named 'providers.shared'
___________ TestAliasResolution.test_anthropic_provider_type_exists ____________
tests/test_model_restrictions.py:611: in test_anthropic_provider_type_exists
    assert hasattr(ProviderType, 'ANTHROPIC'), \
E   AssertionError: ProviderType.ANTHROPIC must exist (FAILS - not in enum yet)
E   assert False
E    +  where False = hasattr(ProviderType, 'ANTHROPIC')
_________________________ test_gemini_provider_imports _________________________
tests/test_gemini_provider.py:27: in test_gemini_provider_imports
    assert 'from google import genai' in source, \
E   AssertionError: Should use new SDK: from google import genai
E   assert 'from google import genai' in 'class GeminiModelProvider(ModelProvider):\n    """Google Gemini model provider implementation."""\n\n    # Model configurations using ModelCapabilities objects\n    SUPPORTED_MODELS = {\n        "gemini-2.0-flash": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.0-flash",\n            friendly_name="Gemini (Flash 2.0)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,  # Experimental thinking mode\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=20.0,  # Conservative 20MB limit for reliability\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=24576,  # Same as 2.5 flash for consistency\n            description="Gemini 2.0 Flash (1M context) - Latest fast model with experimental thinking, supports audio/video input",\n            aliases=["flash-2.0", "flash2"],\n        ),\n        "gemini-2.0-flash-lite": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.0-flash-lite",\n            friendly_name="Gemin (Flash Lite 2.0)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=False,  # Not supported per user request\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=False,  # Does not support images\n            max_image_size_mb=0.0,  # No image support\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            description="Gemini 2.0 Flash Lite (1M context) - Lightweight fast model, text-only",\n            aliases=["flashlite", "flash-lite"],\n        ),\n        "gemini-2.5-flash": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.5-flash",\n            friendly_name="Gemini (Flash 2.5)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=20.0,  # Conservative 20MB limit for reliability\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=24576,  # Flash 2.5 thinking budget limit\n            description="Ultra-fast (1M context) - Quick analysis, simple queries, rapid iterations",\n            aliases=["flash", "flash2.5"],\n        ),\n        "gemini-2.5-pro": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.5-pro",\n            friendly_name="Gemini (Pro 2.5)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=32.0,  # Higher limit for Pro model\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=32768,  # Max thinking tokens for Pro model\n            description="Deep reasoning + thinking mode (1M context) - Complex problems, architecture, deep analysis",\n            aliases=["pro", "gemini pro", "gemini-pro"],\n        ),\n    }\n\n    # Thinking mode configurations - percentages of model\'s max_thinking_tokens\n    # These percentages work across all models that support thinking\n    THINKING_BUDGETS = {\n        "minimal": 0.005,  # 0.5% of max - minimal thinking for fast responses\n        "low": 0.08,  # 8% of max - light reasoning tasks\n        "medium": 0.33,  # 33% of max - balanced reasoning (default)\n        "high": 0.67,  # 67% of max - complex analysis\n        "max": 1.0,  # 100% of max - full thinking budget\n    }\n\n    # Model-specific thinking token limits\n    MAX_THINKING_TOKENS = {\n        "gemini-2.0-flash": 24576,  # Same as 2.5 flash for consistency\n        "gemini-2.0-flash-lite": 0,  # No thinking support\n        "gemini-2.5-flash": 24576,  # Flash 2.5 thinking budget limit\n        "gemini-2.5-pro": 32768,  # Pro 2.5 thinking budget limit\n    }\n\n    def __init__(self, api_key: str, **kwargs):\n        """Initialize Gemini provider with API key."""\n        super().__init__(api_key, **kwargs)\n        self._configured = False\n        self._token_counters = {}  # Cache for token counting\n\n    def _ensure_configured(self):\n        """Ensure Gemini API is configured."""\n        if not self._configured:\n            genai.configure(api_key=self.api_key)\n            self._configured = True\n\n    def get_capabilities(self, model_name: str) -> ModelCapabilities:\n        """Get capabilities for a specific Gemini model."""\n        # Resolve shorthand\n        resolved_name = self._resolve_model_name(model_name)\n\n        if resolved_name not in self.SUPPORTED_MODELS:\n            raise ValueError(f"Unsupported Gemini model: {model_name}")\n\n        # Check if model is allowed by restrictions\n        from utils.model_restrictions import get_restriction_service\n\n        restriction_service = get_restriction_service()\n        # IMPORTANT: Parameter order is (provider_type, model_name, original_name)\n        # resolved_name is the canonical model name, model_name is the user input\n        if not restriction_service.is_allowed(ProviderType.GOOGLE, resolved_name, model_name):\n            raise ValueError(f"Gemini model \'{resolved_name}\' is not allowed by restriction policy.")\n\n        # Return the ModelCapabilities object directly from SUPPORTED_MODELS\n        return self.SUPPORTED_MODELS[resolved_name]\n\n    def generate_content(\n        self,\n        prompt: str,\n        model_name: str,\n        system_prompt: Optional[str] = None,\n        temperature: float = 0.7,\n        max_output_tokens: Optional[int] = None,\n        thinking_mode: str = "medium",\n        images: Optional[list[str]] = None,\n        **kwargs,\n    ) -> ModelResponse:\n        """Generate content using Gemini model."""\n        # Validate parameters\n        resolved_name = self._resolve_model_name(model_name)\n        self.validate_parameters(model_name, temperature)\n\n        # Prepare content parts (text and potentially images)\n        parts = []\n\n        # Add system and user prompts as text\n        if system_prompt:\n            full_prompt = f"{system_prompt}\\n\\n{prompt}"\n        else:\n            full_prompt = prompt\n\n        parts.append({"text": full_prompt})\n\n        # Add images if provided and model supports vision\n        if images and self._supports_vision(resolved_name):\n            for image_path in images:\n                try:\n                    image_part = self._process_image(image_path)\n                    if image_part:\n                        parts.append(image_part)\n                except Exception as e:\n                    logger.warning(f"Failed to process image {image_path}: {e}")\n                    # Continue with other images and text\n                    continue\n        elif images and not self._supports_vision(resolved_name):\n            logger.warning(f"Model {resolved_name} does not support images, ignoring {len(images)} image(s)")\n\n        # Create contents structure\n        contents = [{"parts": parts}]\n\n        # Get capabilities first\n        capabilities = self.get_capabilities(model_name)\n\n        # Prepare generation config\n        generation_config = genai.GenerationConfig(\n            temperature=temperature,\n            candidate_count=1,\n            max_output_tokens=max_output_tokens if max_output_tokens else capabilities.max_output_tokens,\n        )\n\n        # Add thinking configuration for models that support it\n        # Note: The new API doesn\'t support thinking_config yet\n        # TODO: Update when API adds thinking mode support\n        if capabilities.supports_extended_thinking and thinking_mode in self.THINKING_BUDGETS:\n            logger.debug(f"Thinking mode \'{thinking_mode}\' requested but not yet supported in current API")\n\n        # Retry logic with progressive delays\n        max_retries = 4  # Total of 4 attempts\n        retry_delays = [1, 3, 5, 8]  # Progressive delays: 1s, 3s, 5s, 8s\n\n        last_exception = None\n\n        for attempt in range(max_retries):\n            try:\n                # Ensure API is configured\n                self._ensure_configured()\n\n                # Create model instance\n                model = genai.GenerativeModel(\n                    model_name=resolved_name,\n                    generation_config=generation_config,\n                    system_instruction=system_prompt if system_prompt else None,\n                )\n\n                # Generate content\n                response = model.generate_content(contents)\n\n                # Extract usage information if available\n                usage = self._extract_usage(response)\n\n                return ModelResponse(\n                    content=response.text,\n                    usage=usage,\n                    model_name=resolved_name,\n                    friendly_name="Gemini",\n                    provider=ProviderType.GOOGLE,\n                    metadata={\n                        "thinking_mode": thinking_mode if capabilities.supports_extended_thinking else None,\n                        "finish_reason": (\n                            getattr(response.candidates[0], "finish_reason", "STOP") if response.candidates else "STOP"\n                        ),\n                    },\n                )\n\n            except Exception as e:\n                last_exception = e\n\n                # Check if this is a retryable error using structured error codes\n                is_retryable = self._is_error_retryable(e)\n\n                # If this is the last attempt or not retryable, give up\n                if attempt == max_retries - 1 or not is_retryable:\n                    break\n\n                # Get progressive delay\n                delay = retry_delays[attempt]\n\n                # Log retry attempt\n                logger.warning(\n                    f"Gemini API error for model {resolved_name}, attempt {attempt + 1}/{max_retries}: {str(e)}. Retrying in {delay}s..."\n                )\n                time.sleep(delay)\n\n        # If we get here, all retries failed\n        actual_attempts = attempt + 1  # Convert from 0-based index to human-readable count\n        error_msg = f"Gemini API error for model {resolved_name} after {actual_attempts} attempt{\'s\' if actual_attempts > 1 else \'\'}: {str(last_exception)}"\n        raise RuntimeError(error_msg) from last_exception\n\n    def count_tokens(self, text: str, model_name: str) -> int:\n        """Count tokens for the given text using Gemini\'s tokenizer."""\n        self._resolve_model_name(model_name)\n\n        # For now, use a simple estimation\n        # TODO: Use actual Gemini tokenizer when available in SDK\n        # Rough estimation: ~4 characters per token for English text\n        return len(text) // 4\n\n    def get_provider_type(self) -> ProviderType:\n        """Get the provider type."""\n        return ProviderType.GOOGLE\n\n    def validate_model_name(self, model_name: str) -> bool:\n        """Validate if the model name is supported and allowed."""\n        resolved_name = self._resolve_model_name(model_name)\n\n        # First check if model is supported\n        if resolved_name not in self.SUPPORTED_MODELS:\n            return False\n\n        # Then check if model is allowed by restrictions\n        from utils.model_restrictions import get_restriction_service\n\n        restriction_service = get_restriction_service()\n        # IMPORTANT: Parameter order is (provider_type, model_name, original_name)\n        # resolved_name is the canonical model name, model_name is the user input\n        if not restriction_service.is_allowed(ProviderType.GOOGLE, resolved_name, model_name):\n            logger.debug(f"Gemini model \'{model_name}\' -> \'{resolved_name}\' blocked by restrictions")\n            return False\n\n        return True\n\n    def supports_thinking_mode(self, model_name: str) -> bool:\n        """Check if the model supports extended thinking mode."""\n        capabilities = self.get_capabilities(model_name)\n        return capabilities.supports_extended_thinking\n\n    def get_thinking_budget(self, model_name: str, thinking_mode: str) -> int:\n        """Get actual thinking token budget for a model and thinking mode."""\n        resolved_name = self._resolve_model_name(model_name)\n        model_config = self.SUPPORTED_MODELS.get(resolved_name)\n\n        if not model_config or not model_config.supports_extended_thinking:\n            return 0\n\n        if thinking_mode not in self.THINKING_BUDGETS:\n            return 0\n\n        max_thinking_tokens = model_config.max_thinking_tokens\n        if max_thinking_tokens == 0:\n            return 0\n\n        return int(max_thinking_tokens * self.THINKING_BUDGETS[thinking_mode])\n\n    def _extract_usage(self, response) -> dict[str, int]:\n        """Extract token usage from Gemini response."""\n        usage = {}\n\n        # Try to extract usage metadata from response\n        # Note: The actual structure depends on the SDK version and response format\n        if hasattr(response, "usage_metadata"):\n            metadata = response.usage_metadata\n\n            # Extract token counts with explicit None checks\n            input_tokens = None\n            output_tokens = None\n\n            if hasattr(metadata, "prompt_token_count"):\n                value = metadata.prompt_token_count\n                if value is not None:\n                    input_tokens = value\n                    usage["input_tokens"] = value\n\n            if hasattr(metadata, "candidates_token_count"):\n                value = metadata.candidates_token_count\n                if value is not None:\n                    output_tokens = value\n                    usage["output_tokens"] = value\n\n            # Calculate total only if both values are available and valid\n            if input_tokens is not None and output_tokens is not None:\n                usage["total_tokens"] = input_tokens + output_tokens\n\n        return usage\n\n    def _supports_vision(self, model_name: str) -> bool:\n        """Check if the model supports vision (image processing)."""\n        # Gemini 2.5 models support vision\n        vision_models = {\n            "gemini-2.5-flash",\n            "gemini-2.5-pro",\n            "gemini-2.0-flash",\n            "gemini-1.5-pro",\n            "gemini-1.5-flash",\n        }\n        return model_name in vision_models\n\n    def _is_error_retryable(self, error: Exception) -> bool:\n        """Determine if an error should be retried based on structured error codes.\n\n        Uses Gemini API error structure instead of text pattern matching for reliability.\n\n        Args:\n            error: Exception from Gemini API call\n\n        Returns:\n            True if error should be retried, False otherwise\n        """\n        error_str = str(error).lower()\n\n        # Check for 429 errors first - these need special handling\n        if "429" in error_str or "quota" in error_str or "resource_exhausted" in error_str:\n            # For Gemini, check for specific non-retryable error indicators\n            # These typically indicate permanent failures or quota/size limits\n            non_retryable_indicators = [\n                "quota exceeded",\n                "resource exhausted",\n                "context length",\n                "token limit",\n                "request too large",\n                "invalid request",\n                "quota_exceeded",\n                "resource_exhausted",\n            ]\n\n            # Also check if this is a structured error from Gemini SDK\n            try:\n                # Try to access error details if available\n                if hasattr(error, "details") or hasattr(error, "reason"):\n                    # Gemini API errors may have structured details\n                    error_details = getattr(error, "details", "") or getattr(error, "reason", "")\n                    error_details_str = str(error_details).lower()\n\n                    # Check for non-retryable error codes/reasons\n                    if any(indicator in error_details_str for indicator in non_retryable_indicators):\n                        logger.debug(f"Non-retryable Gemini error: {error_details}")\n                        return False\n            except Exception:\n                pass\n\n            # Check main error string for non-retryable patterns\n            if any(indicator in error_str for indicator in non_retryable_indicators):\n                logger.debug(f"Non-retryable Gemini error based on message: {error_str[:200]}...")\n                return False\n\n            # If it\'s a 429/quota error but doesn\'t match non-retryable patterns, it might be retryable rate limiting\n            logger.debug(f"Retryable Gemini rate limiting error: {error_str[:100]}...")\n            return True\n\n        # For non-429 errors, check if they\'re retryable\n        retryable_indicators = [\n            "timeout",\n            "connection",\n            "network",\n            "temporary",\n            "unavailable",\n            "retry",\n            "internal error",\n            "408",  # Request timeout\n            "500",  # Internal server error\n            "502",  # Bad gateway\n            "503",  # Service unavailable\n            "504",  # Gateway timeout\n            "ssl",  # SSL errors\n            "handshake",  # Handshake failures\n        ]\n\n        return any(indicator in error_str for indicator in retryable_indicators)\n\n    def _process_image(self, image_path: str) -> Optional[dict]:\n        """Process an image for Gemini API."""\n        try:\n            if image_path.startswith("data:image/"):\n                # Handle data URL: data:image/png;base64,iVBORw0...\n                header, data = image_path.split(",", 1)\n                mime_type = header.split(";")[0].split(":")[1]\n                return {"inline_data": {"mime_type": mime_type, "data": data}}\n            else:\n                # Handle file path\n                from utils.file_types import get_image_mime_type\n\n                if not os.path.exists(image_path):\n                    logger.warning(f"Image file not found: {image_path}")\n                    return None\n\n                # Detect MIME type from file extension using centralized mappings\n                ext = os.path.splitext(image_path)[1].lower()\n                mime_type = get_image_mime_type(ext)\n\n                # Read and encode the image\n                with open(image_path, "rb") as f:\n                    image_data = base64.b64encode(f.read()).decode()\n\n                return {"inline_data": {"mime_type": mime_type, "data": image_data}}\n        except Exception as e:\n            logger.error(f"Error processing image {image_path}: {e}")\n            return None\n'
_________________ test_gemini_provider_no_deprecated_configure _________________
tests/test_gemini_provider.py:45: in test_gemini_provider_no_deprecated_configure
    assert 'genai.configure(' not in source, \
E   AssertionError: Should NOT use deprecated genai.configure() - use Client() instead
E   assert 'genai.configure(' not in 'class GeminiModelProvider(ModelProvider):\n    """Google Gemini model provider implementation."""\n\n    # Model configurations using ModelCapabilities objects\n    SUPPORTED_MODELS = {\n        "gemini-2.0-flash": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.0-flash",\n            friendly_name="Gemini (Flash 2.0)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,  # Experimental thinking mode\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=20.0,  # Conservative 20MB limit for reliability\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=24576,  # Same as 2.5 flash for consistency\n            description="Gemini 2.0 Flash (1M context) - Latest fast model with experimental thinking, supports audio/video input",\n            aliases=["flash-2.0", "flash2"],\n        ),\n        "gemini-2.0-flash-lite": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.0-flash-lite",\n            friendly_name="Gemin (Flash Lite 2.0)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=False,  # Not supported per user request\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=False,  # Does not support images\n            max_image_size_mb=0.0,  # No image support\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            description="Gemini 2.0 Flash Lite (1M context) - Lightweight fast model, text-only",\n            aliases=["flashlite", "flash-lite"],\n        ),\n        "gemini-2.5-flash": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.5-flash",\n            friendly_name="Gemini (Flash 2.5)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=20.0,  # Conservative 20MB limit for reliability\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=24576,  # Flash 2.5 thinking budget limit\n            description="Ultra-fast (1M context) - Quick analysis, simple queries, rapid iterations",\n            aliases=["flash", "flash2.5"],\n        ),\n        "gemini-2.5-pro": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.5-pro",\n            friendly_name="Gemini (Pro 2.5)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=32.0,  # Higher limit for Pro model\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=32768,  # Max thinking tokens for Pro model\n            description="Deep reasoning + thinking mode (1M context) - Complex problems, architecture, deep analysis",\n            aliases=["pro", "gemini pro", "gemini-pro"],\n        ),\n    }\n\n    # Thinking mode configurations - percentages of model\'s max_thinking_tokens\n    # These percentages work across all models that support thinking\n    THINKING_BUDGETS = {\n        "minimal": 0.005,  # 0.5% of max - minimal thinking for fast responses\n        "low": 0.08,  # 8% of max - light reasoning tasks\n        "medium": 0.33,  # 33% of max - balanced reasoning (default)\n        "high": 0.67,  # 67% of max - complex analysis\n        "max": 1.0,  # 100% of max - full thinking budget\n    }\n\n    # Model-specific thinking token limits\n    MAX_THINKING_TOKENS = {\n        "gemini-2.0-flash": 24576,  # Same as 2.5 flash for consistency\n        "gemini-2.0-flash-lite": 0,  # No thinking support\n        "gemini-2.5-flash": 24576,  # Flash 2.5 thinking budget limit\n        "gemini-2.5-pro": 32768,  # Pro 2.5 thinking budget limit\n    }\n\n    def __init__(self, api_key: str, **kwargs):\n        """Initialize Gemini provider with API key."""\n        super().__init__(api_key, **kwargs)\n        self._configured = False\n        self._token_counters = {}  # Cache for token counting\n\n    def _ensure_configured(self):\n        """Ensure Gemini API is configured."""\n        if not self._configured:\n            genai.configure(api_key=self.api_key)\n            self._configured = True\n\n    def get_capabilities(self, model_name: str) -> ModelCapabilities:\n        """Get capabilities for a specific Gemini model."""\n        # Resolve shorthand\n        resolved_name = self._resolve_model_name(model_name)\n\n        if resolved_name not in self.SUPPORTED_MODELS:\n            raise ValueError(f"Unsupported Gemini model: {model_name}")\n\n        # Check if model is allowed by restrictions\n        from utils.model_restrictions import get_restriction_service\n\n        restriction_service = get_restriction_service()\n        # IMPORTANT: Parameter order is (provider_type, model_name, original_name)\n        # resolved_name is the canonical model name, model_name is the user input\n        if not restriction_service.is_allowed(ProviderType.GOOGLE, resolved_name, model_name):\n            raise ValueError(f"Gemini model \'{resolved_name}\' is not allowed by restriction policy.")\n\n        # Return the ModelCapabilities object directly from SUPPORTED_MODELS\n        return self.SUPPORTED_MODELS[resolved_name]\n\n    def generate_content(\n        self,\n        prompt: str,\n        model_name: str,\n        system_prompt: Optional[str] = None,\n        temperature: float = 0.7,\n        max_output_tokens: Optional[int] = None,\n        thinking_mode: str = "medium",\n        images: Optional[list[str]] = None,\n        **kwargs,\n    ) -> ModelResponse:\n        """Generate content using Gemini model."""\n        # Validate parameters\n        resolved_name = self._resolve_model_name(model_name)\n        self.validate_parameters(model_name, temperature)\n\n        # Prepare content parts (text and potentially images)\n        parts = []\n\n        # Add system and user prompts as text\n        if system_prompt:\n            full_prompt = f"{system_prompt}\\n\\n{prompt}"\n        else:\n            full_prompt = prompt\n\n        parts.append({"text": full_prompt})\n\n        # Add images if provided and model supports vision\n        if images and self._supports_vision(resolved_name):\n            for image_path in images:\n                try:\n                    image_part = self._process_image(image_path)\n                    if image_part:\n                        parts.append(image_part)\n                except Exception as e:\n                    logger.warning(f"Failed to process image {image_path}: {e}")\n                    # Continue with other images and text\n                    continue\n        elif images and not self._supports_vision(resolved_name):\n            logger.warning(f"Model {resolved_name} does not support images, ignoring {len(images)} image(s)")\n\n        # Create contents structure\n        contents = [{"parts": parts}]\n\n        # Get capabilities first\n        capabilities = self.get_capabilities(model_name)\n\n        # Prepare generation config\n        generation_config = genai.GenerationConfig(\n            temperature=temperature,\n            candidate_count=1,\n            max_output_tokens=max_output_tokens if max_output_tokens else capabilities.max_output_tokens,\n        )\n\n        # Add thinking configuration for models that support it\n        # Note: The new API doesn\'t support thinking_config yet\n        # TODO: Update when API adds thinking mode support\n        if capabilities.supports_extended_thinking and thinking_mode in self.THINKING_BUDGETS:\n            logger.debug(f"Thinking mode \'{thinking_mode}\' requested but not yet supported in current API")\n\n        # Retry logic with progressive delays\n        max_retries = 4  # Total of 4 attempts\n        retry_delays = [1, 3, 5, 8]  # Progressive delays: 1s, 3s, 5s, 8s\n\n        last_exception = None\n\n        for attempt in range(max_retries):\n            try:\n                # Ensure API is configured\n                self._ensure_configured()\n\n                # Create model instance\n                model = genai.GenerativeModel(\n                    model_name=resolved_name,\n                    generation_config=generation_config,\n                    system_instruction=system_prompt if system_prompt else None,\n                )\n\n                # Generate content\n                response = model.generate_content(contents)\n\n                # Extract usage information if available\n                usage = self._extract_usage(response)\n\n                return ModelResponse(\n                    content=response.text,\n                    usage=usage,\n                    model_name=resolved_name,\n                    friendly_name="Gemini",\n                    provider=ProviderType.GOOGLE,\n                    metadata={\n                        "thinking_mode": thinking_mode if capabilities.supports_extended_thinking else None,\n                        "finish_reason": (\n                            getattr(response.candidates[0], "finish_reason", "STOP") if response.candidates else "STOP"\n                        ),\n                    },\n                )\n\n            except Exception as e:\n                last_exception = e\n\n                # Check if this is a retryable error using structured error codes\n                is_retryable = self._is_error_retryable(e)\n\n                # If this is the last attempt or not retryable, give up\n                if attempt == max_retries - 1 or not is_retryable:\n                    break\n\n                # Get progressive delay\n                delay = retry_delays[attempt]\n\n                # Log retry attempt\n                logger.warning(\n                    f"Gemini API error for model {resolved_name}, attempt {attempt + 1}/{max_retries}: {str(e)}. Retrying in {delay}s..."\n                )\n                time.sleep(delay)\n\n        # If we get here, all retries failed\n        actual_attempts = attempt + 1  # Convert from 0-based index to human-readable count\n        error_msg = f"Gemini API error for model {resolved_name} after {actual_attempts} attempt{\'s\' if actual_attempts > 1 else \'\'}: {str(last_exception)}"\n        raise RuntimeError(error_msg) from last_exception\n\n    def count_tokens(self, text: str, model_name: str) -> int:\n        """Count tokens for the given text using Gemini\'s tokenizer."""\n        self._resolve_model_name(model_name)\n\n        # For now, use a simple estimation\n        # TODO: Use actual Gemini tokenizer when available in SDK\n        # Rough estimation: ~4 characters per token for English text\n        return len(text) // 4\n\n    def get_provider_type(self) -> ProviderType:\n        """Get the provider type."""\n        return ProviderType.GOOGLE\n\n    def validate_model_name(self, model_name: str) -> bool:\n        """Validate if the model name is supported and allowed."""\n        resolved_name = self._resolve_model_name(model_name)\n\n        # First check if model is supported\n        if resolved_name not in self.SUPPORTED_MODELS:\n            return False\n\n        # Then check if model is allowed by restrictions\n        from utils.model_restrictions import get_restriction_service\n\n        restriction_service = get_restriction_service()\n        # IMPORTANT: Parameter order is (provider_type, model_name, original_name)\n        # resolved_name is the canonical model name, model_name is the user input\n        if not restriction_service.is_allowed(ProviderType.GOOGLE, resolved_name, model_name):\n            logger.debug(f"Gemini model \'{model_name}\' -> \'{resolved_name}\' blocked by restrictions")\n            return False\n\n        return True\n\n    def supports_thinking_mode(self, model_name: str) -> bool:\n        """Check if the model supports extended thinking mode."""\n        capabilities = self.get_capabilities(model_name)\n        return capabilities.supports_extended_thinking\n\n    def get_thinking_budget(self, model_name: str, thinking_mode: str) -> int:\n        """Get actual thinking token budget for a model and thinking mode."""\n        resolved_name = self._resolve_model_name(model_name)\n        model_config = self.SUPPORTED_MODELS.get(resolved_name)\n\n        if not model_config or not model_config.supports_extended_thinking:\n            return 0\n\n        if thinking_mode not in self.THINKING_BUDGETS:\n            return 0\n\n        max_thinking_tokens = model_config.max_thinking_tokens\n        if max_thinking_tokens == 0:\n            return 0\n\n        return int(max_thinking_tokens * self.THINKING_BUDGETS[thinking_mode])\n\n    def _extract_usage(self, response) -> dict[str, int]:\n        """Extract token usage from Gemini response."""\n        usage = {}\n\n        # Try to extract usage metadata from response\n        # Note: The actual structure depends on the SDK version and response format\n        if hasattr(response, "usage_metadata"):\n            metadata = response.usage_metadata\n\n            # Extract token counts with explicit None checks\n            input_tokens = None\n            output_tokens = None\n\n            if hasattr(metadata, "prompt_token_count"):\n                value = metadata.prompt_token_count\n                if value is not None:\n                    input_tokens = value\n                    usage["input_tokens"] = value\n\n            if hasattr(metadata, "candidates_token_count"):\n                value = metadata.candidates_token_count\n                if value is not None:\n                    output_tokens = value\n                    usage["output_tokens"] = value\n\n            # Calculate total only if both values are available and valid\n            if input_tokens is not None and output_tokens is not None:\n                usage["total_tokens"] = input_tokens + output_tokens\n\n        return usage\n\n    def _supports_vision(self, model_name: str) -> bool:\n        """Check if the model supports vision (image processing)."""\n        # Gemini 2.5 models support vision\n        vision_models = {\n            "gemini-2.5-flash",\n            "gemini-2.5-pro",\n            "gemini-2.0-flash",\n            "gemini-1.5-pro",\n            "gemini-1.5-flash",\n        }\n        return model_name in vision_models\n\n    def _is_error_retryable(self, error: Exception) -> bool:\n        """Determine if an error should be retried based on structured error codes.\n\n        Uses Gemini API error structure instead of text pattern matching for reliability.\n\n        Args:\n            error: Exception from Gemini API call\n\n        Returns:\n            True if error should be retried, False otherwise\n        """\n        error_str = str(error).lower()\n\n        # Check for 429 errors first - these need special handling\n        if "429" in error_str or "quota" in error_str or "resource_exhausted" in error_str:\n            # For Gemini, check for specific non-retryable error indicators\n            # These typically indicate permanent failures or quota/size limits\n            non_retryable_indicators = [\n                "quota exceeded",\n                "resource exhausted",\n                "context length",\n                "token limit",\n                "request too large",\n                "invalid request",\n                "quota_exceeded",\n                "resource_exhausted",\n            ]\n\n            # Also check if this is a structured error from Gemini SDK\n            try:\n                # Try to access error details if available\n                if hasattr(error, "details") or hasattr(error, "reason"):\n                    # Gemini API errors may have structured details\n                    error_details = getattr(error, "details", "") or getattr(error, "reason", "")\n                    error_details_str = str(error_details).lower()\n\n                    # Check for non-retryable error codes/reasons\n                    if any(indicator in error_details_str for indicator in non_retryable_indicators):\n                        logger.debug(f"Non-retryable Gemini error: {error_details}")\n                        return False\n            except Exception:\n                pass\n\n            # Check main error string for non-retryable patterns\n            if any(indicator in error_str for indicator in non_retryable_indicators):\n                logger.debug(f"Non-retryable Gemini error based on message: {error_str[:200]}...")\n                return False\n\n            # If it\'s a 429/quota error but doesn\'t match non-retryable patterns, it might be retryable rate limiting\n            logger.debug(f"Retryable Gemini rate limiting error: {error_str[:100]}...")\n            return True\n\n        # For non-429 errors, check if they\'re retryable\n        retryable_indicators = [\n            "timeout",\n            "connection",\n            "network",\n            "temporary",\n            "unavailable",\n            "retry",\n            "internal error",\n            "408",  # Request timeout\n            "500",  # Internal server error\n            "502",  # Bad gateway\n            "503",  # Service unavailable\n            "504",  # Gateway timeout\n            "ssl",  # SSL errors\n            "handshake",  # Handshake failures\n        ]\n\n        return any(indicator in error_str for indicator in retryable_indicators)\n\n    def _process_image(self, image_path: str) -> Optional[dict]:\n        """Process an image for Gemini API."""\n        try:\n            if image_path.startswith("data:image/"):\n                # Handle data URL: data:image/png;base64,iVBORw0...\n                header, data = image_path.split(",", 1)\n                mime_type = header.split(";")[0].split(":")[1]\n                return {"inline_data": {"mime_type": mime_type, "data": data}}\n            else:\n                # Handle file path\n                from utils.file_types import get_image_mime_type\n\n                if not os.path.exists(image_path):\n                    logger.warning(f"Image file not found: {image_path}")\n                    return None\n\n                # Detect MIME type from file extension using centralized mappings\n                ext = os.path.splitext(image_path)[1].lower()\n                mime_type = get_image_mime_type(ext)\n\n                # Read and encode the image\n                with open(image_path, "rb") as f:\n                    image_data = base64.b64encode(f.read()).decode()\n\n                return {"inline_data": {"mime_type": mime_type, "data": image_data}}\n        except Exception as e:\n            logger.error(f"Error processing image {image_path}: {e}")\n            return None\n'
E     
E     'genai.configure(' is contained here:
E       class GeminiModelProvider(ModelProvider):
E           """Google Gemini model provider implementation."""
E       
E           # Model configurations using ModelCapabilities objects
E           SUPPORTED_MODELS = {
E               "gemini-2.0-flash": ModelCapabilities(
E                   provider=ProviderType.GOOGLE,
E                   model_name="gemini-2.0-flash",
E                   friendly_name="Gemini (Flash 2.0)",
E                   context_window=1_048_576,  # 1M tokens
E                   max_output_tokens=65_536,
E                   supports_extended_thinking=True,  # Experimental thinking mode
E                   supports_system_prompts=True,
E                   supports_streaming=True,
E                   supports_function_calling=True,
E                   supports_json_mode=True,
E                   supports_images=True,  # Vision capability
E                   max_image_size_mb=20.0,  # Conservative 20MB limit for reliability
E                   supports_temperature=True,
E                   temperature_constraint=create_temperature_constraint("range"),
E                   max_thinking_tokens=24576,  # Same as 2.5 flash for consistency
E                   description="Gemini 2.0 Flash (1M context) - Latest fast model with experimental thinking, supports audio/video input",
E                   aliases=["flash-2.0", "flash2"],
E               ),
E               "gemini-2.0-flash-lite": ModelCapabilities(
E                   provider=ProviderType.GOOGLE,
E                   model_name="gemini-2.0-flash-lite",
E                   friendly_name="Gemin (Flash Lite 2.0)",
E                   context_window=1_048_576,  # 1M tokens
E                   max_output_tokens=65_536,
E                   supports_extended_thinking=False,  # Not supported per user request
E                   supports_system_prompts=True,
E                   supports_streaming=True,
E                   supports_function_calling=True,
E                   supports_json_mode=True,
E                   supports_images=False,  # Does not support images
E                   max_image_size_mb=0.0,  # No image support
E                   supports_temperature=True,
E                   temperature_constraint=create_temperature_constraint("range"),
E                   description="Gemini 2.0 Flash Lite (1M context) - Lightweight fast model, text-only",
E                   aliases=["flashlite", "flash-lite"],
E               ),
E               "gemini-2.5-flash": ModelCapabilities(
E                   provider=ProviderType.GOOGLE,
E                   model_name="gemini-2.5-flash",
E                   friendly_name="Gemini (Flash 2.5)",
E                   context_window=1_048_576,  # 1M tokens
E                   max_output_tokens=65_536,
E                   supports_extended_thinking=True,
E                   supports_system_prompts=True,
E                   supports_streaming=True,
E                   supports_function_calling=True,
E                   supports_json_mode=True,
E                   supports_images=True,  # Vision capability
E                   max_image_size_mb=20.0,  # Conservative 20MB limit for reliability
E                   supports_temperature=True,
E                   temperature_constraint=create_temperature_constraint("range"),
E                   max_thinking_tokens=24576,  # Flash 2.5 thinking budget limit
E                   description="Ultra-fast (1M context) - Quick analysis, simple queries, rapid iterations",
E                   aliases=["flash", "flash2.5"],
E               ),
E               "gemini-2.5-pro": ModelCapabilities(
E                   provider=ProviderType.GOOGLE,
E                   model_name="gemini-2.5-pro",
E                   friendly_name="Gemini (Pro 2.5)",
E                   context_window=1_048_576,  # 1M tokens
E                   max_output_tokens=65_536,
E                   supports_extended_thinking=True,
E                   supports_system_prompts=True,
E                   supports_streaming=True,
E                   supports_function_calling=True,
E                   supports_json_mode=True,
E                   supports_images=True,  # Vision capability
E                   max_image_size_mb=32.0,  # Higher limit for Pro model
E                   supports_temperature=True,
E                   temperature_constraint=create_temperature_constraint("range"),
E                   max_thinking_tokens=32768,  # Max thinking tokens for Pro model
E                   description="Deep reasoning + thinking mode (1M context) - Complex problems, architecture, deep analysis",
E                   aliases=["pro", "gemini pro", "gemini-pro"],
E               ),
E           }
E       
E           # Thinking mode configurations - percentages of model's max_thinking_tokens
E           # These percentages work across all models that support thinking
E           THINKING_BUDGETS = {
E               "minimal": 0.005,  # 0.5% of max - minimal thinking for fast responses
E               "low": 0.08,  # 8% of max - light reasoning tasks
E               "medium": 0.33,  # 33% of max - balanced reasoning (default)
E               "high": 0.67,  # 67% of max - complex analysis
E               "max": 1.0,  # 100% of max - full thinking budget
E           }
E       
E           # Model-specific thinking token limits
E           MAX_THINKING_TOKENS = {
E               "gemini-2.0-flash": 24576,  # Same as 2.5 flash for consistency
E               "gemini-2.0-flash-lite": 0,  # No thinking support
E               "gemini-2.5-flash": 24576,  # Flash 2.5 thinking budget limit
E               "gemini-2.5-pro": 32768,  # Pro 2.5 thinking budget limit
E           }
E       
E           def __init__(self, api_key: str, **kwargs):
E               """Initialize Gemini provider with API key."""
E               super().__init__(api_key, **kwargs)
E               self._configured = False
E               self._token_counters = {}  # Cache for token counting
E       
E           def _ensure_configured(self):
E               """Ensure Gemini API is configured."""
E               if not self._configured:
E                   genai.configure(api_key=self.api_key)
E     ?             ++++++++++++++++
E                   self._configured = True
E       
E           def get_capabilities(self, model_name: str) -> ModelCapabilities:
E               """Get capabilities for a specific Gemini model."""
E               # Resolve shorthand
E               resolved_name = self._resolve_model_name(model_name)
E       
E               if resolved_name not in self.SUPPORTED_MODELS:
E                   raise ValueError(f"Unsupported Gemini model: {model_name}")
E       
E               # Check if model is allowed by restrictions
E               from utils.model_restrictions import get_restriction_service
E       
E               restriction_service = get_restriction_service()
E               # IMPORTANT: Parameter order is (provider_type, model_name, original_name)
E               # resolved_name is the canonical model name, model_name is the user input
E               if not restriction_service.is_allowed(ProviderType.GOOGLE, resolved_name, model_name):
E                   raise ValueError(f"Gemini model '{resolved_name}' is not allowed by restriction policy.")
E       
E               # Return the ModelCapabilities object directly from SUPPORTED_MODELS
E               return self.SUPPORTED_MODELS[resolved_name]
E       
E           def generate_content(
E               self,
E               prompt: str,
E               model_name: str,
E               system_prompt: Optional[str] = None,
E               temperature: float = 0.7,
E               max_output_tokens: Optional[int] = None,
E               thinking_mode: str = "medium",
E               images: Optional[list[str]] = None,
E               **kwargs,
E           ) -> ModelResponse:
E               """Generate content using Gemini model."""
E               # Validate parameters
E               resolved_name = self._resolve_model_name(model_name)
E               self.validate_parameters(model_name, temperature)
E       
E               # Prepare content parts (text and potentially images)
E               parts = []
E       
E               # Add system and user prompts as text
E               if system_prompt:
E                   full_prompt = f"{system_prompt}\n\n{prompt}"
E               else:
E                   full_prompt = prompt
E       
E               parts.append({"text": full_prompt})
E       
E               # Add images if provided and model supports vision
E               if images and self._supports_vision(resolved_name):
E                   for image_path in images:
E                       try:
E                           image_part = self._process_image(image_path)
E                           if image_part:
E                               parts.append(image_part)
E                       except Exception as e:
E                           logger.warning(f"Failed to process image {image_path}: {e}")
E                           # Continue with other images and text
E                           continue
E               elif images and not self._supports_vision(resolved_name):
E                   logger.warning(f"Model {resolved_name} does not support images, ignoring {len(images)} image(s)")
E       
E               # Create contents structure
E               contents = [{"parts": parts}]
E       
E               # Get capabilities first
E               capabilities = self.get_capabilities(model_name)
E       
E               # Prepare generation config
E               generation_config = genai.GenerationConfig(
E                   temperature=temperature,
E                   candidate_count=1,
E                   max_output_tokens=max_output_tokens if max_output_tokens else capabilities.max_output_tokens,
E               )
E       
E               # Add thinking configuration for models that support it
E               # Note: The new API doesn't support thinking_config yet
E               # TODO: Update when API adds thinking mode support
E               if capabilities.supports_extended_thinking and thinking_mode in self.THINKING_BUDGETS:
E                   logger.debug(f"Thinking mode '{thinking_mode}' requested but not yet supported in current API")
E       
E               # Retry logic with progressive delays
E               max_retries = 4  # Total of 4 attempts
E               retry_delays = [1, 3, 5, 8]  # Progressive delays: 1s, 3s, 5s, 8s
E       
E               last_exception = None
E       
E               for attempt in range(max_retries):
E                   try:
E                       # Ensure API is configured
E                       self._ensure_configured()
E       
E                       # Create model instance
E                       model = genai.GenerativeModel(
E                           model_name=resolved_name,
E                           generation_config=generation_config,
E                           system_instruction=system_prompt if system_prompt else None,
E                       )
E       
E                       # Generate content
E                       response = model.generate_content(contents)
E       
E                       # Extract usage information if available
E                       usage = self._extract_usage(response)
E       
E                       return ModelResponse(
E                           content=response.text,
E                           usage=usage,
E                           model_name=resolved_name,
E                           friendly_name="Gemini",
E                           provider=ProviderType.GOOGLE,
E                           metadata={
E                               "thinking_mode": thinking_mode if capabilities.supports_extended_thinking else None,
E                               "finish_reason": (
E                                   getattr(response.candidates[0], "finish_reason", "STOP") if response.candidates else "STOP"
E                               ),
E                           },
E                       )
E       
E                   except Exception as e:
E                       last_exception = e
E       
E                       # Check if this is a retryable error using structured error codes
E                       is_retryable = self._is_error_retryable(e)
E       
E                       # If this is the last attempt or not retryable, give up
E                       if attempt == max_retries - 1 or not is_retryable:
E                           break
E       
E                       # Get progressive delay
E                       delay = retry_delays[attempt]
E       
E                       # Log retry attempt
E                       logger.warning(
E                           f"Gemini API error for model {resolved_name}, attempt {attempt + 1}/{max_retries}: {str(e)}. Retrying in {delay}s..."
E                       )
E                       time.sleep(delay)
E       
E               # If we get here, all retries failed
E               actual_attempts = attempt + 1  # Convert from 0-based index to human-readable count
E               error_msg = f"Gemini API error for model {resolved_name} after {actual_attempts} attempt{'s' if actual_attempts > 1 else ''}: {str(last_exception)}"
E               raise RuntimeError(error_msg) from last_exception
E       
E           def count_tokens(self, text: str, model_name: str) -> int:
E               """Count tokens for the given text using Gemini's tokenizer."""
E               self._resolve_model_name(model_name)
E       
E               # For now, use a simple estimation
E               # TODO: Use actual Gemini tokenizer when available in SDK
E               # Rough estimation: ~4 characters per token for English text
E               return len(text) // 4
E       
E           def get_provider_type(self) -> ProviderType:
E               """Get the provider type."""
E               return ProviderType.GOOGLE
E       
E           def validate_model_name(self, model_name: str) -> bool:
E               """Validate if the model name is supported and allowed."""
E               resolved_name = self._resolve_model_name(model_name)
E       
E               # First check if model is supported
E               if resolved_name not in self.SUPPORTED_MODELS:
E                   return False
E       
E               # Then check if model is allowed by restrictions
E               from utils.model_restrictions import get_restriction_service
E       
E               restriction_service = get_restriction_service()
E               # IMPORTANT: Parameter order is (provider_type, model_name, original_name)
E               # resolved_name is the canonical model name, model_name is the user input
E               if not restriction_service.is_allowed(ProviderType.GOOGLE, resolved_name, model_name):
E                   logger.debug(f"Gemini model '{model_name}' -> '{resolved_name}' blocked by restrictions")
E                   return False
E       
E               return True
E       
E           def supports_thinking_mode(self, model_name: str) -> bool:
E               """Check if the model supports extended thinking mode."""
E               capabilities = self.get_capabilities(model_name)
E               return capabilities.supports_extended_thinking
E       
E           def get_thinking_budget(self, model_name: str, thinking_mode: str) -> int:
E               """Get actual thinking token budget for a model and thinking mode."""
E               resolved_name = self._resolve_model_name(model_name)
E               model_config = self.SUPPORTED_MODELS.get(resolved_name)
E       
E               if not model_config or not model_config.supports_extended_thinking:
E                   return 0
E       
E               if thinking_mode not in self.THINKING_BUDGETS:
E                   return 0
E       
E               max_thinking_tokens = model_config.max_thinking_tokens
E               if max_thinking_tokens == 0:
E                   return 0
E       
E               return int(max_thinking_tokens * self.THINKING_BUDGETS[thinking_mode])
E       
E           def _extract_usage(self, response) -> dict[str, int]:
E               """Extract token usage from Gemini response."""
E               usage = {}
E       
E               # Try to extract usage metadata from response
E               # Note: The actual structure depends on the SDK version and response format
E               if hasattr(response, "usage_metadata"):
E                   metadata = response.usage_metadata
E       
E                   # Extract token counts with explicit None checks
E                   input_tokens = None
E                   output_tokens = None
E       
E                   if hasattr(metadata, "prompt_token_count"):
E                       value = metadata.prompt_token_count
E                       if value is not None:
E                           input_tokens = value
E                           usage["input_tokens"] = value
E       
E                   if hasattr(metadata, "candidates_token_count"):
E                       value = metadata.candidates_token_count
E                       if value is not None:
E                           output_tokens = value
E                           usage["output_tokens"] = value
E       
E                   # Calculate total only if both values are available and valid
E                   if input_tokens is not None and output_tokens is not None:
E                       usage["total_tokens"] = input_tokens + output_tokens
E       
E               return usage
E       
E           def _supports_vision(self, model_name: str) -> bool:
E               """Check if the model supports vision (image processing)."""
E               # Gemini 2.5 models support vision
E               vision_models = {
E                   "gemini-2.5-flash",
E                   "gemini-2.5-pro",
E                   "gemini-2.0-flash",
E                   "gemini-1.5-pro",
E                   "gemini-1.5-flash",
E               }
E               return model_name in vision_models
E       
E           def _is_error_retryable(self, error: Exception) -> bool:
E               """Determine if an error should be retried based on structured error codes.
E       
E               Uses Gemini API error structure instead of text pattern matching for reliability.
E       
E               Args:
E                   error: Exception from Gemini API call
E       
E               Returns:
E                   True if error should be retried, False otherwise
E               """
E               error_str = str(error).lower()
E       
E               # Check for 429 errors first - these need special handling
E               if "429" in error_str or "quota" in error_str or "resource_exhausted" in error_str:
E                   # For Gemini, check for specific non-retryable error indicators
E                   # These typically indicate permanent failures or quota/size limits
E                   non_retryable_indicators = [
E                       "quota exceeded",
E                       "resource exhausted",
E                       "context length",
E                       "token limit",
E                       "request too large",
E                       "invalid request",
E                       "quota_exceeded",
E                       "resource_exhausted",
E                   ]
E       
E                   # Also check if this is a structured error from Gemini SDK
E                   try:
E                       # Try to access error details if available
E                       if hasattr(error, "details") or hasattr(error, "reason"):
E                           # Gemini API errors may have structured details
E                           error_details = getattr(error, "details", "") or getattr(error, "reason", "")
E                           error_details_str = str(error_details).lower()
E       
E                           # Check for non-retryable error codes/reasons
E                           if any(indicator in error_details_str for indicator in non_retryable_indicators):
E                               logger.debug(f"Non-retryable Gemini error: {error_details}")
E                               return False
E                   except Exception:
E                       pass
E       
E                   # Check main error string for non-retryable patterns
E                   if any(indicator in error_str for indicator in non_retryable_indicators):
E                       logger.debug(f"Non-retryable Gemini error based on message: {error_str[:200]}...")
E                       return False
E       
E                   # If it's a 429/quota error but doesn't match non-retryable patterns, it might be retryable rate limiting
E                   logger.debug(f"Retryable Gemini rate limiting error: {error_str[:100]}...")
E                   return True
E       
E               # For non-429 errors, check if they're retryable
E               retryable_indicators = [
E                   "timeout",
E                   "connection",
E                   "network",
E                   "temporary",
E                   "unavailable",
E                   "retry",
E                   "internal error",
E                   "408",  # Request timeout
E                   "500",  # Internal server error
E                   "502",  # Bad gateway
E                   "503",  # Service unavailable
E                   "504",  # Gateway timeout
E                   "ssl",  # SSL errors
E                   "handshake",  # Handshake failures
E               ]
E       
E               return any(indicator in error_str for indicator in retryable_indicators)
E       
E           def _process_image(self, image_path: str) -> Optional[dict]:
E               """Process an image for Gemini API."""
E               try:
E                   if image_path.startswith("data:image/"):
E                       # Handle data URL: data:image/png;base64,iVBORw0...
E                       header, data = image_path.split(",", 1)
E                       mime_type = header.split(";")[0].split(":")[1]
E                       return {"inline_data": {"mime_type": mime_type, "data": data}}
E                   else:
E                       # Handle file path
E                       from utils.file_types import get_image_mime_type
E       
E                       if not os.path.exists(image_path):
E                           logger.warning(f"Image file not found: {image_path}")
E                           return None
E       
E                       # Detect MIME type from file extension using centralized mappings
E                       ext = os.path.splitext(image_path)[1].lower()
E                       mime_type = get_image_mime_type(ext)
E       
E                       # Read and encode the image
E                       with open(image_path, "rb") as f:
E                           image_data = base64.b64encode(f.read()).decode()
E       
E                       return {"inline_data": {"mime_type": mime_type, "data": image_data}}
E               except Exception as e:
E                   logger.error(f"Error processing image {image_path}: {e}")
E                   return None
___________________ test_gemini_provider_uses_client_pattern ___________________
tests/test_gemini_provider.py:59: in test_gemini_provider_uses_client_pattern
    assert 'genai.Client(' in source or 'Client(' in source, \
E   AssertionError: Should use new Client() pattern for SDK initialization
E   assert ('genai.Client(' in 'class GeminiModelProvider(ModelProvider):\n    """Google Gemini model provider implementation."""\n\n    # Model configurations using ModelCapabilities objects\n    SUPPORTED_MODELS = {\n        "gemini-2.0-flash": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.0-flash",\n            friendly_name="Gemini (Flash 2.0)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,  # Experimental thinking mode\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=20.0,  # Conservative 20MB limit for reliability\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=24576,  # Same as 2.5 flash for consistency\n            description="Gemini 2.0 Flash (1M context) - Latest fast model with experimental thinking, supports audio/video input",\n            aliases=["flash-2.0", "flash2"],\n        ),\n        "gemini-2.0-flash-lite": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.0-flash-lite",\n            friendly_name="Gemin (Flash Lite 2.0)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=False,  # Not supported per user request\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=False,  # Does not support images\n            max_image_size_mb=0.0,  # No image support\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            description="Gemini 2.0 Flash Lite (1M context) - Lightweight fast model, text-only",\n            aliases=["flashlite", "flash-lite"],\n        ),\n        "gemini-2.5-flash": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.5-flash",\n            friendly_name="Gemini (Flash 2.5)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=20.0,  # Conservative 20MB limit for reliability\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=24576,  # Flash 2.5 thinking budget limit\n            description="Ultra-fast (1M context) - Quick analysis, simple queries, rapid iterations",\n            aliases=["flash", "flash2.5"],\n        ),\n        "gemini-2.5-pro": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.5-pro",\n            friendly_name="Gemini (Pro 2.5)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=32.0,  # Higher limit for Pro model\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=32768,  # Max thinking tokens for Pro model\n            description="Deep reasoning + thinking mode (1M context) - Complex problems, architecture, deep analysis",\n            aliases=["pro", "gemini pro", "gemini-pro"],\n        ),\n    }\n\n    # Thinking mode configurations - percentages of model\'s max_thinking_tokens\n    # These percentages work across all models that support thinking\n    THINKING_BUDGETS = {\n        "minimal": 0.005,  # 0.5% of max - minimal thinking for fast responses\n        "low": 0.08,  # 8% of max - light reasoning tasks\n        "medium": 0.33,  # 33% of max - balanced reasoning (default)\n        "high": 0.67,  # 67% of max - complex analysis\n        "max": 1.0,  # 100% of max - full thinking budget\n    }\n\n    # Model-specific thinking token limits\n    MAX_THINKING_TOKENS = {\n        "gemini-2.0-flash": 24576,  # Same as 2.5 flash for consistency\n        "gemini-2.0-flash-lite": 0,  # No thinking support\n        "gemini-2.5-flash": 24576,  # Flash 2.5 thinking budget limit\n        "gemini-2.5-pro": 32768,  # Pro 2.5 thinking budget limit\n    }\n\n    def __init__(self, api_key: str, **kwargs):\n        """Initialize Gemini provider with API key."""\n        super().__init__(api_key, **kwargs)\n        self._configured = False\n        self._token_counters = {}  # Cache for token counting\n\n    def _ensure_configured(self):\n        """Ensure Gemini API is configured."""\n        if not self._configured:\n            genai.configure(api_key=self.api_key)\n            self._configured = True\n\n    def get_capabilities(self, model_name: str) -> ModelCapabilities:\n        """Get capabilities for a specific Gemini model."""\n        # Resolve shorthand\n        resolved_name = self._resolve_model_name(model_name)\n\n        if resolved_name not in self.SUPPORTED_MODELS:\n            raise ValueError(f"Unsupported Gemini model: {model_name}")\n\n        # Check if model is allowed by restrictions\n        from utils.model_restrictions import get_restriction_service\n\n        restriction_service = get_restriction_service()\n        # IMPORTANT: Parameter order is (provider_type, model_name, original_name)\n        # resolved_name is the canonical model name, model_name is the user input\n        if not restriction_service.is_allowed(ProviderType.GOOGLE, resolved_name, model_name):\n            raise ValueError(f"Gemini model \'{resolved_name}\' is not allowed by restriction policy.")\n\n        # Return the ModelCapabilities object directly from SUPPORTED_MODELS\n        return self.SUPPORTED_MODELS[resolved_name]\n\n    def generate_content(\n        self,\n        prompt: str,\n        model_name: str,\n        system_prompt: Optional[str] = None,\n        temperature: float = 0.7,\n        max_output_tokens: Optional[int] = None,\n        thinking_mode: str = "medium",\n        images: Optional[list[str]] = None,\n        **kwargs,\n    ) -> ModelResponse:\n        """Generate content using Gemini model."""\n        # Validate parameters\n        resolved_name = self._resolve_model_name(model_name)\n        self.validate_parameters(model_name, temperature)\n\n        # Prepare content parts (text and potentially images)\n        parts = []\n\n        # Add system and user prompts as text\n        if system_prompt:\n            full_prompt = f"{system_prompt}\\n\\n{prompt}"\n        else:\n            full_prompt = prompt\n\n        parts.append({"text": full_prompt})\n\n        # Add images if provided and model supports vision\n        if images and self._supports_vision(resolved_name):\n            for image_path in images:\n                try:\n                    image_part = self._process_image(image_path)\n                    if image_part:\n                        parts.append(image_part)\n                except Exception as e:\n                    logger.warning(f"Failed to process image {image_path}: {e}")\n                    # Continue with other images and text\n                    continue\n        elif images and not self._supports_vision(resolved_name):\n            logger.warning(f"Model {resolved_name} does not support images, ignoring {len(images)} image(s)")\n\n        # Create contents structure\n        contents = [{"parts": parts}]\n\n        # Get capabilities first\n        capabilities = self.get_capabilities(model_name)\n\n        # Prepare generation config\n        generation_config = genai.GenerationConfig(\n            temperature=temperature,\n            candidate_count=1,\n            max_output_tokens=max_output_tokens if max_output_tokens else capabilities.max_output_tokens,\n        )\n\n        # Add thinking configuration for models that support it\n        # Note: The new API doesn\'t support thinking_config yet\n        # TODO: Update when API adds thinking mode support\n        if capabilities.supports_extended_thinking and thinking_mode in self.THINKING_BUDGETS:\n            logger.debug(f"Thinking mode \'{thinking_mode}\' requested but not yet supported in current API")\n\n        # Retry logic with progressive delays\n        max_retries = 4  # Total of 4 attempts\n        retry_delays = [1, 3, 5, 8]  # Progressive delays: 1s, 3s, 5s, 8s\n\n        last_exception = None\n\n        for attempt in range(max_retries):\n            try:\n                # Ensure API is configured\n                self._ensure_configured()\n\n                # Create model instance\n                model = genai.GenerativeModel(\n                    model_name=resolved_name,\n                    generation_config=generation_config,\n                    system_instruction=system_prompt if system_prompt else None,\n                )\n\n                # Generate content\n                response = model.generate_content(contents)\n\n                # Extract usage information if available\n                usage = self._extract_usage(response)\n\n                return ModelResponse(\n                    content=response.text,\n                    usage=usage,\n                    model_name=resolved_name,\n                    friendly_name="Gemini",\n                    provider=ProviderType.GOOGLE,\n                    metadata={\n                        "thinking_mode": thinking_mode if capabilities.supports_extended_thinking else None,\n                        "finish_reason": (\n                            getattr(response.candidates[0], "finish_reason", "STOP") if response.candidates else "STOP"\n                        ),\n                    },\n                )\n\n            except Exception as e:\n                last_exception = e\n\n                # Check if this is a retryable error using structured error codes\n                is_retryable = self._is_error_retryable(e)\n\n                # If this is the last attempt or not retryable, give up\n                if attempt == max_retries - 1 or not is_retryable:\n                    break\n\n                # Get progressive delay\n                delay = retry_delays[attempt]\n\n                # Log retry attempt\n                logger.warning(\n                    f"Gemini API error for model {resolved_name}, attempt {attempt + 1}/{max_retries}: {str(e)}. Retrying in {delay}s..."\n                )\n                time.sleep(delay)\n\n        # If we get here, all retries failed\n        actual_attempts = attempt + 1  # Convert from 0-based index to human-readable count\n        error_msg = f"Gemini API error for model {resolved_name} after {actual_attempts} attempt{\'s\' if actual_attempts > 1 else \'\'}: {str(last_exception)}"\n        raise RuntimeError(error_msg) from last_exception\n\n    def count_tokens(self, text: str, model_name: str) -> int:\n        """Count tokens for the given text using Gemini\'s tokenizer."""\n        self._resolve_model_name(model_name)\n\n        # For now, use a simple estimation\n        # TODO: Use actual Gemini tokenizer when available in SDK\n        # Rough estimation: ~4 characters per token for English text\n        return len(text) // 4\n\n    def get_provider_type(self) -> ProviderType:\n        """Get the provider type."""\n        return ProviderType.GOOGLE\n\n    def validate_model_name(self, model_name: str) -> bool:\n        """Validate if the model name is supported and allowed."""\n        resolved_name = self._resolve_model_name(model_name)\n\n        # First check if model is supported\n        if resolved_name not in self.SUPPORTED_MODELS:\n            return False\n\n        # Then check if model is allowed by restrictions\n        from utils.model_restrictions import get_restriction_service\n\n        restriction_service = get_restriction_service()\n        # IMPORTANT: Parameter order is (provider_type, model_name, original_name)\n        # resolved_name is the canonical model name, model_name is the user input\n        if not restriction_service.is_allowed(ProviderType.GOOGLE, resolved_name, model_name):\n            logger.debug(f"Gemini model \'{model_name}\' -> \'{resolved_name}\' blocked by restrictions")\n            return False\n\n        return True\n\n    def supports_thinking_mode(self, model_name: str) -> bool:\n        """Check if the model supports extended thinking mode."""\n        capabilities = self.get_capabilities(model_name)\n        return capabilities.supports_extended_thinking\n\n    def get_thinking_budget(self, model_name: str, thinking_mode: str) -> int:\n        """Get actual thinking token budget for a model and thinking mode."""\n        resolved_name = self._resolve_model_name(model_name)\n        model_config = self.SUPPORTED_MODELS.get(resolved_name)\n\n        if not model_config or not model_config.supports_extended_thinking:\n            return 0\n\n        if thinking_mode not in self.THINKING_BUDGETS:\n            return 0\n\n        max_thinking_tokens = model_config.max_thinking_tokens\n        if max_thinking_tokens == 0:\n            return 0\n\n        return int(max_thinking_tokens * self.THINKING_BUDGETS[thinking_mode])\n\n    def _extract_usage(self, response) -> dict[str, int]:\n        """Extract token usage from Gemini response."""\n        usage = {}\n\n        # Try to extract usage metadata from response\n        # Note: The actual structure depends on the SDK version and response format\n        if hasattr(response, "usage_metadata"):\n            metadata = response.usage_metadata\n\n            # Extract token counts with explicit None checks\n            input_tokens = None\n            output_tokens = None\n\n            if hasattr(metadata, "prompt_token_count"):\n                value = metadata.prompt_token_count\n                if value is not None:\n                    input_tokens = value\n                    usage["input_tokens"] = value\n\n            if hasattr(metadata, "candidates_token_count"):\n                value = metadata.candidates_token_count\n                if value is not None:\n                    output_tokens = value\n                    usage["output_tokens"] = value\n\n            # Calculate total only if both values are available and valid\n            if input_tokens is not None and output_tokens is not None:\n                usage["total_tokens"] = input_tokens + output_tokens\n\n        return usage\n\n    def _supports_vision(self, model_name: str) -> bool:\n        """Check if the model supports vision (image processing)."""\n        # Gemini 2.5 models support vision\n        vision_models = {\n            "gemini-2.5-flash",\n            "gemini-2.5-pro",\n            "gemini-2.0-flash",\n            "gemini-1.5-pro",\n            "gemini-1.5-flash",\n        }\n        return model_name in vision_models\n\n    def _is_error_retryable(self, error: Exception) -> bool:\n        """Determine if an error should be retried based on structured error codes.\n\n        Uses Gemini API error structure instead of text pattern matching for reliability.\n\n        Args:\n            error: Exception from Gemini API call\n\n        Returns:\n            True if error should be retried, False otherwise\n        """\n        error_str = str(error).lower()\n\n        # Check for 429 errors first - these need special handling\n        if "429" in error_str or "quota" in error_str or "resource_exhausted" in error_str:\n            # For Gemini, check for specific non-retryable error indicators\n            # These typically indicate permanent failures or quota/size limits\n            non_retryable_indicators = [\n                "quota exceeded",\n                "resource exhausted",\n                "context length",\n                "token limit",\n                "request too large",\n                "invalid request",\n                "quota_exceeded",\n                "resource_exhausted",\n            ]\n\n            # Also check if this is a structured error from Gemini SDK\n            try:\n                # Try to access error details if available\n                if hasattr(error, "details") or hasattr(error, "reason"):\n                    # Gemini API errors may have structured details\n                    error_details = getattr(error, "details", "") or getattr(error, "reason", "")\n                    error_details_str = str(error_details).lower()\n\n                    # Check for non-retryable error codes/reasons\n                    if any(indicator in error_details_str for indicator in non_retryable_indicators):\n                        logger.debug(f"Non-retryable Gemini error: {error_details}")\n                        return False\n            except Exception:\n                pass\n\n            # Check main error string for non-retryable patterns\n            if any(indicator in error_str for indicator in non_retryable_indicators):\n                logger.debug(f"Non-retryable Gemini error based on message: {error_str[:200]}...")\n                return False\n\n            # If it\'s a 429/quota error but doesn\'t match non-retryable patterns, it might be retryable rate limiting\n            logger.debug(f"Retryable Gemini rate limiting error: {error_str[:100]}...")\n            return True\n\n        # For non-429 errors, check if they\'re retryable\n        retryable_indicators = [\n            "timeout",\n            "connection",\n            "network",\n            "temporary",\n            "unavailable",\n            "retry",\n            "internal error",\n            "408",  # Request timeout\n            "500",  # Internal server error\n            "502",  # Bad gateway\n            "503",  # Service unavailable\n            "504",  # Gateway timeout\n            "ssl",  # SSL errors\n            "handshake",  # Handshake failures\n        ]\n\n        return any(indicator in error_str for indicator in retryable_indicators)\n\n    def _process_image(self, image_path: str) -> Optional[dict]:\n        """Process an image for Gemini API."""\n        try:\n            if image_path.startswith("data:image/"):\n                # Handle data URL: data:image/png;base64,iVBORw0...\n                header, data = image_path.split(",", 1)\n                mime_type = header.split(";")[0].split(":")[1]\n                return {"inline_data": {"mime_type": mime_type, "data": data}}\n            else:\n                # Handle file path\n                from utils.file_types import get_image_mime_type\n\n                if not os.path.exists(image_path):\n                    logger.warning(f"Image file not found: {image_path}")\n                    return None\n\n                # Detect MIME type from file extension using centralized mappings\n                ext = os.path.splitext(image_path)[1].lower()\n                mime_type = get_image_mime_type(ext)\n\n                # Read and encode the image\n                with open(image_path, "rb") as f:\n                    image_data = base64.b64encode(f.read()).decode()\n\n                return {"inline_data": {"mime_type": mime_type, "data": image_data}}\n        except Exception as e:\n            logger.error(f"Error processing image {image_path}: {e}")\n            return None\n' or 'Client(' in 'class GeminiModelProvider(ModelProvider):\n    """Google Gemini model provider implementation."""\n\n    # Model configurations using ModelCapabilities objects\n    SUPPORTED_MODELS = {\n        "gemini-2.0-flash": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.0-flash",\n            friendly_name="Gemini (Flash 2.0)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,  # Experimental thinking mode\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=20.0,  # Conservative 20MB limit for reliability\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=24576,  # Same as 2.5 flash for consistency\n            description="Gemini 2.0 Flash (1M context) - Latest fast model with experimental thinking, supports audio/video input",\n            aliases=["flash-2.0", "flash2"],\n        ),\n        "gemini-2.0-flash-lite": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.0-flash-lite",\n            friendly_name="Gemin (Flash Lite 2.0)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=False,  # Not supported per user request\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=False,  # Does not support images\n            max_image_size_mb=0.0,  # No image support\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            description="Gemini 2.0 Flash Lite (1M context) - Lightweight fast model, text-only",\n            aliases=["flashlite", "flash-lite"],\n        ),\n        "gemini-2.5-flash": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.5-flash",\n            friendly_name="Gemini (Flash 2.5)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=20.0,  # Conservative 20MB limit for reliability\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=24576,  # Flash 2.5 thinking budget limit\n            description="Ultra-fast (1M context) - Quick analysis, simple queries, rapid iterations",\n            aliases=["flash", "flash2.5"],\n        ),\n        "gemini-2.5-pro": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.5-pro",\n            friendly_name="Gemini (Pro 2.5)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=32.0,  # Higher limit for Pro model\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=32768,  # Max thinking tokens for Pro model\n            description="Deep reasoning + thinking mode (1M context) - Complex problems, architecture, deep analysis",\n            aliases=["pro", "gemini pro", "gemini-pro"],\n        ),\n    }\n\n    # Thinking mode configurations - percentages of model\'s max_thinking_tokens\n    # These percentages work across all models that support thinking\n    THINKING_BUDGETS = {\n        "minimal": 0.005,  # 0.5% of max - minimal thinking for fast responses\n        "low": 0.08,  # 8% of max - light reasoning tasks\n        "medium": 0.33,  # 33% of max - balanced reasoning (default)\n        "high": 0.67,  # 67% of max - complex analysis\n        "max": 1.0,  # 100% of max - full thinking budget\n    }\n\n    # Model-specific thinking token limits\n    MAX_THINKING_TOKENS = {\n        "gemini-2.0-flash": 24576,  # Same as 2.5 flash for consistency\n        "gemini-2.0-flash-lite": 0,  # No thinking support\n        "gemini-2.5-flash": 24576,  # Flash 2.5 thinking budget limit\n        "gemini-2.5-pro": 32768,  # Pro 2.5 thinking budget limit\n    }\n\n    def __init__(self, api_key: str, **kwargs):\n        """Initialize Gemini provider with API key."""\n        super().__init__(api_key, **kwargs)\n        self._configured = False\n        self._token_counters = {}  # Cache for token counting\n\n    def _ensure_configured(self):\n        """Ensure Gemini API is configured."""\n        if not self._configured:\n            genai.configure(api_key=self.api_key)\n            self._configured = True\n\n    def get_capabilities(self, model_name: str) -> ModelCapabilities:\n        """Get capabilities for a specific Gemini model."""\n        # Resolve shorthand\n        resolved_name = self._resolve_model_name(model_name)\n\n        if resolved_name not in self.SUPPORTED_MODELS:\n            raise ValueError(f"Unsupported Gemini model: {model_name}")\n\n        # Check if model is allowed by restrictions\n        from utils.model_restrictions import get_restriction_service\n\n        restriction_service = get_restriction_service()\n        # IMPORTANT: Parameter order is (provider_type, model_name, original_name)\n        # resolved_name is the canonical model name, model_name is the user input\n        if not restriction_service.is_allowed(ProviderType.GOOGLE, resolved_name, model_name):\n            raise ValueError(f"Gemini model \'{resolved_name}\' is not allowed by restriction policy.")\n\n        # Return the ModelCapabilities object directly from SUPPORTED_MODELS\n        return self.SUPPORTED_MODELS[resolved_name]\n\n    def generate_content(\n        self,\n        prompt: str,\n        model_name: str,\n        system_prompt: Optional[str] = None,\n        temperature: float = 0.7,\n        max_output_tokens: Optional[int] = None,\n        thinking_mode: str = "medium",\n        images: Optional[list[str]] = None,\n        **kwargs,\n    ) -> ModelResponse:\n        """Generate content using Gemini model."""\n        # Validate parameters\n        resolved_name = self._resolve_model_name(model_name)\n        self.validate_parameters(model_name, temperature)\n\n        # Prepare content parts (text and potentially images)\n        parts = []\n\n        # Add system and user prompts as text\n        if system_prompt:\n            full_prompt = f"{system_prompt}\\n\\n{prompt}"\n        else:\n            full_prompt = prompt\n\n        parts.append({"text": full_prompt})\n\n        # Add images if provided and model supports vision\n        if images and self._supports_vision(resolved_name):\n            for image_path in images:\n                try:\n                    image_part = self._process_image(image_path)\n                    if image_part:\n                        parts.append(image_part)\n                except Exception as e:\n                    logger.warning(f"Failed to process image {image_path}: {e}")\n                    # Continue with other images and text\n                    continue\n        elif images and not self._supports_vision(resolved_name):\n            logger.warning(f"Model {resolved_name} does not support images, ignoring {len(images)} image(s)")\n\n        # Create contents structure\n        contents = [{"parts": parts}]\n\n        # Get capabilities first\n        capabilities = self.get_capabilities(model_name)\n\n        # Prepare generation config\n        generation_config = genai.GenerationConfig(\n            temperature=temperature,\n            candidate_count=1,\n            max_output_tokens=max_output_tokens if max_output_tokens else capabilities.max_output_tokens,\n        )\n\n        # Add thinking configuration for models that support it\n        # Note: The new API doesn\'t support thinking_config yet\n        # TODO: Update when API adds thinking mode support\n        if capabilities.supports_extended_thinking and thinking_mode in self.THINKING_BUDGETS:\n            logger.debug(f"Thinking mode \'{thinking_mode}\' requested but not yet supported in current API")\n\n        # Retry logic with progressive delays\n        max_retries = 4  # Total of 4 attempts\n        retry_delays = [1, 3, 5, 8]  # Progressive delays: 1s, 3s, 5s, 8s\n\n        last_exception = None\n\n        for attempt in range(max_retries):\n            try:\n                # Ensure API is configured\n                self._ensure_configured()\n\n                # Create model instance\n                model = genai.GenerativeModel(\n                    model_name=resolved_name,\n                    generation_config=generation_config,\n                    system_instruction=system_prompt if system_prompt else None,\n                )\n\n                # Generate content\n                response = model.generate_content(contents)\n\n                # Extract usage information if available\n                usage = self._extract_usage(response)\n\n                return ModelResponse(\n                    content=response.text,\n                    usage=usage,\n                    model_name=resolved_name,\n                    friendly_name="Gemini",\n                    provider=ProviderType.GOOGLE,\n                    metadata={\n                        "thinking_mode": thinking_mode if capabilities.supports_extended_thinking else None,\n                        "finish_reason": (\n                            getattr(response.candidates[0], "finish_reason", "STOP") if response.candidates else "STOP"\n                        ),\n                    },\n                )\n\n            except Exception as e:\n                last_exception = e\n\n                # Check if this is a retryable error using structured error codes\n                is_retryable = self._is_error_retryable(e)\n\n                # If this is the last attempt or not retryable, give up\n                if attempt == max_retries - 1 or not is_retryable:\n                    break\n\n                # Get progressive delay\n                delay = retry_delays[attempt]\n\n                # Log retry attempt\n                logger.warning(\n                    f"Gemini API error for model {resolved_name}, attempt {attempt + 1}/{max_retries}: {str(e)}. Retrying in {delay}s..."\n                )\n                time.sleep(delay)\n\n        # If we get here, all retries failed\n        actual_attempts = attempt + 1  # Convert from 0-based index to human-readable count\n        error_msg = f"Gemini API error for model {resolved_name} after {actual_attempts} attempt{\'s\' if actual_attempts > 1 else \'\'}: {str(last_exception)}"\n        raise RuntimeError(error_msg) from last_exception\n\n    def count_tokens(self, text: str, model_name: str) -> int:\n        """Count tokens for the given text using Gemini\'s tokenizer."""\n        self._resolve_model_name(model_name)\n\n        # For now, use a simple estimation\n        # TODO: Use actual Gemini tokenizer when available in SDK\n        # Rough estimation: ~4 characters per token for English text\n        return len(text) // 4\n\n    def get_provider_type(self) -> ProviderType:\n        """Get the provider type."""\n        return ProviderType.GOOGLE\n\n    def validate_model_name(self, model_name: str) -> bool:\n        """Validate if the model name is supported and allowed."""\n        resolved_name = self._resolve_model_name(model_name)\n\n        # First check if model is supported\n        if resolved_name not in self.SUPPORTED_MODELS:\n            return False\n\n        # Then check if model is allowed by restrictions\n        from utils.model_restrictions import get_restriction_service\n\n        restriction_service = get_restriction_service()\n        # IMPORTANT: Parameter order is (provider_type, model_name, original_name)\n        # resolved_name is the canonical model name, model_name is the user input\n        if not restriction_service.is_allowed(ProviderType.GOOGLE, resolved_name, model_name):\n            logger.debug(f"Gemini model \'{model_name}\' -> \'{resolved_name}\' blocked by restrictions")\n            return False\n\n        return True\n\n    def supports_thinking_mode(self, model_name: str) -> bool:\n        """Check if the model supports extended thinking mode."""\n        capabilities = self.get_capabilities(model_name)\n        return capabilities.supports_extended_thinking\n\n    def get_thinking_budget(self, model_name: str, thinking_mode: str) -> int:\n        """Get actual thinking token budget for a model and thinking mode."""\n        resolved_name = self._resolve_model_name(model_name)\n        model_config = self.SUPPORTED_MODELS.get(resolved_name)\n\n        if not model_config or not model_config.supports_extended_thinking:\n            return 0\n\n        if thinking_mode not in self.THINKING_BUDGETS:\n            return 0\n\n        max_thinking_tokens = model_config.max_thinking_tokens\n        if max_thinking_tokens == 0:\n            return 0\n\n        return int(max_thinking_tokens * self.THINKING_BUDGETS[thinking_mode])\n\n    def _extract_usage(self, response) -> dict[str, int]:\n        """Extract token usage from Gemini response."""\n        usage = {}\n\n        # Try to extract usage metadata from response\n        # Note: The actual structure depends on the SDK version and response format\n        if hasattr(response, "usage_metadata"):\n            metadata = response.usage_metadata\n\n            # Extract token counts with explicit None checks\n            input_tokens = None\n            output_tokens = None\n\n            if hasattr(metadata, "prompt_token_count"):\n                value = metadata.prompt_token_count\n                if value is not None:\n                    input_tokens = value\n                    usage["input_tokens"] = value\n\n            if hasattr(metadata, "candidates_token_count"):\n                value = metadata.candidates_token_count\n                if value is not None:\n                    output_tokens = value\n                    usage["output_tokens"] = value\n\n            # Calculate total only if both values are available and valid\n            if input_tokens is not None and output_tokens is not None:\n                usage["total_tokens"] = input_tokens + output_tokens\n\n        return usage\n\n    def _supports_vision(self, model_name: str) -> bool:\n        """Check if the model supports vision (image processing)."""\n        # Gemini 2.5 models support vision\n        vision_models = {\n            "gemini-2.5-flash",\n            "gemini-2.5-pro",\n            "gemini-2.0-flash",\n            "gemini-1.5-pro",\n            "gemini-1.5-flash",\n        }\n        return model_name in vision_models\n\n    def _is_error_retryable(self, error: Exception) -> bool:\n        """Determine if an error should be retried based on structured error codes.\n\n        Uses Gemini API error structure instead of text pattern matching for reliability.\n\n        Args:\n            error: Exception from Gemini API call\n\n        Returns:\n            True if error should be retried, False otherwise\n        """\n        error_str = str(error).lower()\n\n        # Check for 429 errors first - these need special handling\n        if "429" in error_str or "quota" in error_str or "resource_exhausted" in error_str:\n            # For Gemini, check for specific non-retryable error indicators\n            # These typically indicate permanent failures or quota/size limits\n            non_retryable_indicators = [\n                "quota exceeded",\n                "resource exhausted",\n                "context length",\n                "token limit",\n                "request too large",\n                "invalid request",\n                "quota_exceeded",\n                "resource_exhausted",\n            ]\n\n            # Also check if this is a structured error from Gemini SDK\n            try:\n                # Try to access error details if available\n                if hasattr(error, "details") or hasattr(error, "reason"):\n                    # Gemini API errors may have structured details\n                    error_details = getattr(error, "details", "") or getattr(error, "reason", "")\n                    error_details_str = str(error_details).lower()\n\n                    # Check for non-retryable error codes/reasons\n                    if any(indicator in error_details_str for indicator in non_retryable_indicators):\n                        logger.debug(f"Non-retryable Gemini error: {error_details}")\n                        return False\n            except Exception:\n                pass\n\n            # Check main error string for non-retryable patterns\n            if any(indicator in error_str for indicator in non_retryable_indicators):\n                logger.debug(f"Non-retryable Gemini error based on message: {error_str[:200]}...")\n                return False\n\n            # If it\'s a 429/quota error but doesn\'t match non-retryable patterns, it might be retryable rate limiting\n            logger.debug(f"Retryable Gemini rate limiting error: {error_str[:100]}...")\n            return True\n\n        # For non-429 errors, check if they\'re retryable\n        retryable_indicators = [\n            "timeout",\n            "connection",\n            "network",\n            "temporary",\n            "unavailable",\n            "retry",\n            "internal error",\n            "408",  # Request timeout\n            "500",  # Internal server error\n            "502",  # Bad gateway\n            "503",  # Service unavailable\n            "504",  # Gateway timeout\n            "ssl",  # SSL errors\n            "handshake",  # Handshake failures\n        ]\n\n        return any(indicator in error_str for indicator in retryable_indicators)\n\n    def _process_image(self, image_path: str) -> Optional[dict]:\n        """Process an image for Gemini API."""\n        try:\n            if image_path.startswith("data:image/"):\n                # Handle data URL: data:image/png;base64,iVBORw0...\n                header, data = image_path.split(",", 1)\n                mime_type = header.split(";")[0].split(":")[1]\n                return {"inline_data": {"mime_type": mime_type, "data": data}}\n            else:\n                # Handle file path\n                from utils.file_types import get_image_mime_type\n\n                if not os.path.exists(image_path):\n                    logger.warning(f"Image file not found: {image_path}")\n                    return None\n\n                # Detect MIME type from file extension using centralized mappings\n                ext = os.path.splitext(image_path)[1].lower()\n                mime_type = get_image_mime_type(ext)\n\n                # Read and encode the image\n                with open(image_path, "rb") as f:\n                    image_data = base64.b64encode(f.read()).decode()\n\n                return {"inline_data": {"mime_type": mime_type, "data": image_data}}\n        except Exception as e:\n            logger.error(f"Error processing image {image_path}: {e}")\n            return None\n')
=========================== short test summary info ============================
FAILED tests/test_provider_shared.py::test_providertype_import - ModuleNotFoundError: No module named 'providers.shared'
FAILED tests/test_provider_shared.py::test_providertype_values - ModuleNotFoundError: No module named 'providers.shared'
FAILED tests/test_model_restrictions.py::TestAliasResolution::test_anthropic_provider_type_exists - AssertionError: ProviderType.ANTHROPIC must exist (FAILS - not in enum yet)
assert False
 +  where False = hasattr(ProviderType, 'ANTHROPIC')
FAILED tests/test_gemini_provider.py::test_gemini_provider_imports - AssertionError: Should use new SDK: from google import genai
assert 'from google import genai' in 'class GeminiModelProvider(ModelProvider):\n    """Google Gemini model provider implementation."""\n\n    # Model configurations using ModelCapabilities objects\n    SUPPORTED_MODELS = {\n        "gemini-2.0-flash": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.0-flash",\n            friendly_name="Gemini (Flash 2.0)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,  # Experimental thinking mode\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=20.0,  # Conservative 20MB limit for reliability\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=24576,  # Same as 2.5 flash for consistency\n            description="Gemini 2.0 Flash (1M context) - Latest fast model with experimental thinking, supports audio/video input",\n            aliases=["flash-2.0", "flash2"],\n        ),\n        "gemini-2.0-flash-lite": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.0-flash-lite",\n            friendly_name="Gemin (Flash Lite 2.0)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=False,  # Not supported per user request\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=False,  # Does not support images\n            max_image_size_mb=0.0,  # No image support\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            description="Gemini 2.0 Flash Lite (1M context) - Lightweight fast model, text-only",\n            aliases=["flashlite", "flash-lite"],\n        ),\n        "gemini-2.5-flash": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.5-flash",\n            friendly_name="Gemini (Flash 2.5)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=20.0,  # Conservative 20MB limit for reliability\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=24576,  # Flash 2.5 thinking budget limit\n            description="Ultra-fast (1M context) - Quick analysis, simple queries, rapid iterations",\n            aliases=["flash", "flash2.5"],\n        ),\n        "gemini-2.5-pro": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.5-pro",\n            friendly_name="Gemini (Pro 2.5)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=32.0,  # Higher limit for Pro model\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=32768,  # Max thinking tokens for Pro model\n            description="Deep reasoning + thinking mode (1M context) - Complex problems, architecture, deep analysis",\n            aliases=["pro", "gemini pro", "gemini-pro"],\n        ),\n    }\n\n    # Thinking mode configurations - percentages of model\'s max_thinking_tokens\n    # These percentages work across all models that support thinking\n    THINKING_BUDGETS = {\n        "minimal": 0.005,  # 0.5% of max - minimal thinking for fast responses\n        "low": 0.08,  # 8% of max - light reasoning tasks\n        "medium": 0.33,  # 33% of max - balanced reasoning (default)\n        "high": 0.67,  # 67% of max - complex analysis\n        "max": 1.0,  # 100% of max - full thinking budget\n    }\n\n    # Model-specific thinking token limits\n    MAX_THINKING_TOKENS = {\n        "gemini-2.0-flash": 24576,  # Same as 2.5 flash for consistency\n        "gemini-2.0-flash-lite": 0,  # No thinking support\n        "gemini-2.5-flash": 24576,  # Flash 2.5 thinking budget limit\n        "gemini-2.5-pro": 32768,  # Pro 2.5 thinking budget limit\n    }\n\n    def __init__(self, api_key: str, **kwargs):\n        """Initialize Gemini provider with API key."""\n        super().__init__(api_key, **kwargs)\n        self._configured = False\n        self._token_counters = {}  # Cache for token counting\n\n    def _ensure_configured(self):\n        """Ensure Gemini API is configured."""\n        if not self._configured:\n            genai.configure(api_key=self.api_key)\n            self._configured = True\n\n    def get_capabilities(self, model_name: str) -> ModelCapabilities:\n        """Get capabilities for a specific Gemini model."""\n        # Resolve shorthand\n        resolved_name = self._resolve_model_name(model_name)\n\n        if resolved_name not in self.SUPPORTED_MODELS:\n            raise ValueError(f"Unsupported Gemini model: {model_name}")\n\n        # Check if model is allowed by restrictions\n        from utils.model_restrictions import get_restriction_service\n\n        restriction_service = get_restriction_service()\n        # IMPORTANT: Parameter order is (provider_type, model_name, original_name)\n        # resolved_name is the canonical model name, model_name is the user input\n        if not restriction_service.is_allowed(ProviderType.GOOGLE, resolved_name, model_name):\n            raise ValueError(f"Gemini model \'{resolved_name}\' is not allowed by restriction policy.")\n\n        # Return the ModelCapabilities object directly from SUPPORTED_MODELS\n        return self.SUPPORTED_MODELS[resolved_name]\n\n    def generate_content(\n        self,\n        prompt: str,\n        model_name: str,\n        system_prompt: Optional[str] = None,\n        temperature: float = 0.7,\n        max_output_tokens: Optional[int] = None,\n        thinking_mode: str = "medium",\n        images: Optional[list[str]] = None,\n        **kwargs,\n    ) -> ModelResponse:\n        """Generate content using Gemini model."""\n        # Validate parameters\n        resolved_name = self._resolve_model_name(model_name)\n        self.validate_parameters(model_name, temperature)\n\n        # Prepare content parts (text and potentially images)\n        parts = []\n\n        # Add system and user prompts as text\n        if system_prompt:\n            full_prompt = f"{system_prompt}\\n\\n{prompt}"\n        else:\n            full_prompt = prompt\n\n        parts.append({"text": full_prompt})\n\n        # Add images if provided and model supports vision\n        if images and self._supports_vision(resolved_name):\n            for image_path in images:\n                try:\n                    image_part = self._process_image(image_path)\n                    if image_part:\n                        parts.append(image_part)\n                except Exception as e:\n                    logger.warning(f"Failed to process image {image_path}: {e}")\n                    # Continue with other images and text\n                    continue\n        elif images and not self._supports_vision(resolved_name):\n            logger.warning(f"Model {resolved_name} does not support images, ignoring {len(images)} image(s)")\n\n        # Create contents structure\n        contents = [{"parts": parts}]\n\n        # Get capabilities first\n        capabilities = self.get_capabilities(model_name)\n\n        # Prepare generation config\n        generation_config = genai.GenerationConfig(\n            temperature=temperature,\n            candidate_count=1,\n            max_output_tokens=max_output_tokens if max_output_tokens else capabilities.max_output_tokens,\n        )\n\n        # Add thinking configuration for models that support it\n        # Note: The new API doesn\'t support thinking_config yet\n        # TODO: Update when API adds thinking mode support\n        if capabilities.supports_extended_thinking and thinking_mode in self.THINKING_BUDGETS:\n            logger.debug(f"Thinking mode \'{thinking_mode}\' requested but not yet supported in current API")\n\n        # Retry logic with progressive delays\n        max_retries = 4  # Total of 4 attempts\n        retry_delays = [1, 3, 5, 8]  # Progressive delays: 1s, 3s, 5s, 8s\n\n        last_exception = None\n\n        for attempt in range(max_retries):\n            try:\n                # Ensure API is configured\n                self._ensure_configured()\n\n                # Create model instance\n                model = genai.GenerativeModel(\n                    model_name=resolved_name,\n                    generation_config=generation_config,\n                    system_instruction=system_prompt if system_prompt else None,\n                )\n\n                # Generate content\n                response = model.generate_content(contents)\n\n                # Extract usage information if available\n                usage = self._extract_usage(response)\n\n                return ModelResponse(\n                    content=response.text,\n                    usage=usage,\n                    model_name=resolved_name,\n                    friendly_name="Gemini",\n                    provider=ProviderType.GOOGLE,\n                    metadata={\n                        "thinking_mode": thinking_mode if capabilities.supports_extended_thinking else None,\n                        "finish_reason": (\n                            getattr(response.candidates[0], "finish_reason", "STOP") if response.candidates else "STOP"\n                        ),\n                    },\n                )\n\n            except Exception as e:\n                last_exception = e\n\n                # Check if this is a retryable error using structured error codes\n                is_retryable = self._is_error_retryable(e)\n\n                # If this is the last attempt or not retryable, give up\n                if attempt == max_retries - 1 or not is_retryable:\n                    break\n\n                # Get progressive delay\n                delay = retry_delays[attempt]\n\n                # Log retry attempt\n                logger.warning(\n                    f"Gemini API error for model {resolved_name}, attempt {attempt + 1}/{max_retries}: {str(e)}. Retrying in {delay}s..."\n                )\n                time.sleep(delay)\n\n        # If we get here, all retries failed\n        actual_attempts = attempt + 1  # Convert from 0-based index to human-readable count\n        error_msg = f"Gemini API error for model {resolved_name} after {actual_attempts} attempt{\'s\' if actual_attempts > 1 else \'\'}: {str(last_exception)}"\n        raise RuntimeError(error_msg) from last_exception\n\n    def count_tokens(self, text: str, model_name: str) -> int:\n        """Count tokens for the given text using Gemini\'s tokenizer."""\n        self._resolve_model_name(model_name)\n\n        # For now, use a simple estimation\n        # TODO: Use actual Gemini tokenizer when available in SDK\n        # Rough estimation: ~4 characters per token for English text\n        return len(text) // 4\n\n    def get_provider_type(self) -> ProviderType:\n        """Get the provider type."""\n        return ProviderType.GOOGLE\n\n    def validate_model_name(self, model_name: str) -> bool:\n        """Validate if the model name is supported and allowed."""\n        resolved_name = self._resolve_model_name(model_name)\n\n        # First check if model is supported\n        if resolved_name not in self.SUPPORTED_MODELS:\n            return False\n\n        # Then check if model is allowed by restrictions\n        from utils.model_restrictions import get_restriction_service\n\n        restriction_service = get_restriction_service()\n        # IMPORTANT: Parameter order is (provider_type, model_name, original_name)\n        # resolved_name is the canonical model name, model_name is the user input\n        if not restriction_service.is_allowed(ProviderType.GOOGLE, resolved_name, model_name):\n            logger.debug(f"Gemini model \'{model_name}\' -> \'{resolved_name}\' blocked by restrictions")\n            return False\n\n        return True\n\n    def supports_thinking_mode(self, model_name: str) -> bool:\n        """Check if the model supports extended thinking mode."""\n        capabilities = self.get_capabilities(model_name)\n        return capabilities.supports_extended_thinking\n\n    def get_thinking_budget(self, model_name: str, thinking_mode: str) -> int:\n        """Get actual thinking token budget for a model and thinking mode."""\n        resolved_name = self._resolve_model_name(model_name)\n        model_config = self.SUPPORTED_MODELS.get(resolved_name)\n\n        if not model_config or not model_config.supports_extended_thinking:\n            return 0\n\n        if thinking_mode not in self.THINKING_BUDGETS:\n            return 0\n\n        max_thinking_tokens = model_config.max_thinking_tokens\n        if max_thinking_tokens == 0:\n            return 0\n\n        return int(max_thinking_tokens * self.THINKING_BUDGETS[thinking_mode])\n\n    def _extract_usage(self, response) -> dict[str, int]:\n        """Extract token usage from Gemini response."""\n        usage = {}\n\n        # Try to extract usage metadata from response\n        # Note: The actual structure depends on the SDK version and response format\n        if hasattr(response, "usage_metadata"):\n            metadata = response.usage_metadata\n\n            # Extract token counts with explicit None checks\n            input_tokens = None\n            output_tokens = None\n\n            if hasattr(metadata, "prompt_token_count"):\n                value = metadata.prompt_token_count\n                if value is not None:\n                    input_tokens = value\n                    usage["input_tokens"] = value\n\n            if hasattr(metadata, "candidates_token_count"):\n                value = metadata.candidates_token_count\n                if value is not None:\n                    output_tokens = value\n                    usage["output_tokens"] = value\n\n            # Calculate total only if both values are available and valid\n            if input_tokens is not None and output_tokens is not None:\n                usage["total_tokens"] = input_tokens + output_tokens\n\n        return usage\n\n    def _supports_vision(self, model_name: str) -> bool:\n        """Check if the model supports vision (image processing)."""\n        # Gemini 2.5 models support vision\n        vision_models = {\n            "gemini-2.5-flash",\n            "gemini-2.5-pro",\n            "gemini-2.0-flash",\n            "gemini-1.5-pro",\n            "gemini-1.5-flash",\n        }\n        return model_name in vision_models\n\n    def _is_error_retryable(self, error: Exception) -> bool:\n        """Determine if an error should be retried based on structured error codes.\n\n        Uses Gemini API error structure instead of text pattern matching for reliability.\n\n        Args:\n            error: Exception from Gemini API call\n\n        Returns:\n            True if error should be retried, False otherwise\n        """\n        error_str = str(error).lower()\n\n        # Check for 429 errors first - these need special handling\n        if "429" in error_str or "quota" in error_str or "resource_exhausted" in error_str:\n            # For Gemini, check for specific non-retryable error indicators\n            # These typically indicate permanent failures or quota/size limits\n            non_retryable_indicators = [\n                "quota exceeded",\n                "resource exhausted",\n                "context length",\n                "token limit",\n                "request too large",\n                "invalid request",\n                "quota_exceeded",\n                "resource_exhausted",\n            ]\n\n            # Also check if this is a structured error from Gemini SDK\n            try:\n                # Try to access error details if available\n                if hasattr(error, "details") or hasattr(error, "reason"):\n                    # Gemini API errors may have structured details\n                    error_details = getattr(error, "details", "") or getattr(error, "reason", "")\n                    error_details_str = str(error_details).lower()\n\n                    # Check for non-retryable error codes/reasons\n                    if any(indicator in error_details_str for indicator in non_retryable_indicators):\n                        logger.debug(f"Non-retryable Gemini error: {error_details}")\n                        return False\n            except Exception:\n                pass\n\n            # Check main error string for non-retryable patterns\n            if any(indicator in error_str for indicator in non_retryable_indicators):\n                logger.debug(f"Non-retryable Gemini error based on message: {error_str[:200]}...")\n                return False\n\n            # If it\'s a 429/quota error but doesn\'t match non-retryable patterns, it might be retryable rate limiting\n            logger.debug(f"Retryable Gemini rate limiting error: {error_str[:100]}...")\n            return True\n\n        # For non-429 errors, check if they\'re retryable\n        retryable_indicators = [\n            "timeout",\n            "connection",\n            "network",\n            "temporary",\n            "unavailable",\n            "retry",\n            "internal error",\n            "408",  # Request timeout\n            "500",  # Internal server error\n            "502",  # Bad gateway\n            "503",  # Service unavailable\n            "504",  # Gateway timeout\n            "ssl",  # SSL errors\n            "handshake",  # Handshake failures\n        ]\n\n        return any(indicator in error_str for indicator in retryable_indicators)\n\n    def _process_image(self, image_path: str) -> Optional[dict]:\n        """Process an image for Gemini API."""\n        try:\n            if image_path.startswith("data:image/"):\n                # Handle data URL: data:image/png;base64,iVBORw0...\n                header, data = image_path.split(",", 1)\n                mime_type = header.split(";")[0].split(":")[1]\n                return {"inline_data": {"mime_type": mime_type, "data": data}}\n            else:\n                # Handle file path\n                from utils.file_types import get_image_mime_type\n\n                if not os.path.exists(image_path):\n                    logger.warning(f"Image file not found: {image_path}")\n                    return None\n\n                # Detect MIME type from file extension using centralized mappings\n                ext = os.path.splitext(image_path)[1].lower()\n                mime_type = get_image_mime_type(ext)\n\n                # Read and encode the image\n                with open(image_path, "rb") as f:\n                    image_data = base64.b64encode(f.read()).decode()\n\n                return {"inline_data": {"mime_type": mime_type, "data": image_data}}\n        except Exception as e:\n            logger.error(f"Error processing image {image_path}: {e}")\n            return None\n'
FAILED tests/test_gemini_provider.py::test_gemini_provider_no_deprecated_configure - AssertionError: Should NOT use deprecated genai.configure() - use Client() instead
assert 'genai.configure(' not in 'class GeminiModelProvider(ModelProvider):\n    """Google Gemini model provider implementation."""\n\n    # Model configurations using ModelCapabilities objects\n    SUPPORTED_MODELS = {\n        "gemini-2.0-flash": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.0-flash",\n            friendly_name="Gemini (Flash 2.0)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,  # Experimental thinking mode\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=20.0,  # Conservative 20MB limit for reliability\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=24576,  # Same as 2.5 flash for consistency\n            description="Gemini 2.0 Flash (1M context) - Latest fast model with experimental thinking, supports audio/video input",\n            aliases=["flash-2.0", "flash2"],\n        ),\n        "gemini-2.0-flash-lite": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.0-flash-lite",\n            friendly_name="Gemin (Flash Lite 2.0)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=False,  # Not supported per user request\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=False,  # Does not support images\n            max_image_size_mb=0.0,  # No image support\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            description="Gemini 2.0 Flash Lite (1M context) - Lightweight fast model, text-only",\n            aliases=["flashlite", "flash-lite"],\n        ),\n        "gemini-2.5-flash": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.5-flash",\n            friendly_name="Gemini (Flash 2.5)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=20.0,  # Conservative 20MB limit for reliability\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=24576,  # Flash 2.5 thinking budget limit\n            description="Ultra-fast (1M context) - Quick analysis, simple queries, rapid iterations",\n            aliases=["flash", "flash2.5"],\n        ),\n        "gemini-2.5-pro": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.5-pro",\n            friendly_name="Gemini (Pro 2.5)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=32.0,  # Higher limit for Pro model\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=32768,  # Max thinking tokens for Pro model\n            description="Deep reasoning + thinking mode (1M context) - Complex problems, architecture, deep analysis",\n            aliases=["pro", "gemini pro", "gemini-pro"],\n        ),\n    }\n\n    # Thinking mode configurations - percentages of model\'s max_thinking_tokens\n    # These percentages work across all models that support thinking\n    THINKING_BUDGETS = {\n        "minimal": 0.005,  # 0.5% of max - minimal thinking for fast responses\n        "low": 0.08,  # 8% of max - light reasoning tasks\n        "medium": 0.33,  # 33% of max - balanced reasoning (default)\n        "high": 0.67,  # 67% of max - complex analysis\n        "max": 1.0,  # 100% of max - full thinking budget\n    }\n\n    # Model-specific thinking token limits\n    MAX_THINKING_TOKENS = {\n        "gemini-2.0-flash": 24576,  # Same as 2.5 flash for consistency\n        "gemini-2.0-flash-lite": 0,  # No thinking support\n        "gemini-2.5-flash": 24576,  # Flash 2.5 thinking budget limit\n        "gemini-2.5-pro": 32768,  # Pro 2.5 thinking budget limit\n    }\n\n    def __init__(self, api_key: str, **kwargs):\n        """Initialize Gemini provider with API key."""\n        super().__init__(api_key, **kwargs)\n        self._configured = False\n        self._token_counters = {}  # Cache for token counting\n\n    def _ensure_configured(self):\n        """Ensure Gemini API is configured."""\n        if not self._configured:\n            genai.configure(api_key=self.api_key)\n            self._configured = True\n\n    def get_capabilities(self, model_name: str) -> ModelCapabilities:\n        """Get capabilities for a specific Gemini model."""\n        # Resolve shorthand\n        resolved_name = self._resolve_model_name(model_name)\n\n        if resolved_name not in self.SUPPORTED_MODELS:\n            raise ValueError(f"Unsupported Gemini model: {model_name}")\n\n        # Check if model is allowed by restrictions\n        from utils.model_restrictions import get_restriction_service\n\n        restriction_service = get_restriction_service()\n        # IMPORTANT: Parameter order is (provider_type, model_name, original_name)\n        # resolved_name is the canonical model name, model_name is the user input\n        if not restriction_service.is_allowed(ProviderType.GOOGLE, resolved_name, model_name):\n            raise ValueError(f"Gemini model \'{resolved_name}\' is not allowed by restriction policy.")\n\n        # Return the ModelCapabilities object directly from SUPPORTED_MODELS\n        return self.SUPPORTED_MODELS[resolved_name]\n\n    def generate_content(\n        self,\n        prompt: str,\n        model_name: str,\n        system_prompt: Optional[str] = None,\n        temperature: float = 0.7,\n        max_output_tokens: Optional[int] = None,\n        thinking_mode: str = "medium",\n        images: Optional[list[str]] = None,\n        **kwargs,\n    ) -> ModelResponse:\n        """Generate content using Gemini model."""\n        # Validate parameters\n        resolved_name = self._resolve_model_name(model_name)\n        self.validate_parameters(model_name, temperature)\n\n        # Prepare content parts (text and potentially images)\n        parts = []\n\n        # Add system and user prompts as text\n        if system_prompt:\n            full_prompt = f"{system_prompt}\\n\\n{prompt}"\n        else:\n            full_prompt = prompt\n\n        parts.append({"text": full_prompt})\n\n        # Add images if provided and model supports vision\n        if images and self._supports_vision(resolved_name):\n            for image_path in images:\n                try:\n                    image_part = self._process_image(image_path)\n                    if image_part:\n                        parts.append(image_part)\n                except Exception as e:\n                    logger.warning(f"Failed to process image {image_path}: {e}")\n                    # Continue with other images and text\n                    continue\n        elif images and not self._supports_vision(resolved_name):\n            logger.warning(f"Model {resolved_name} does not support images, ignoring {len(images)} image(s)")\n\n        # Create contents structure\n        contents = [{"parts": parts}]\n\n        # Get capabilities first\n        capabilities = self.get_capabilities(model_name)\n\n        # Prepare generation config\n        generation_config = genai.GenerationConfig(\n            temperature=temperature,\n            candidate_count=1,\n            max_output_tokens=max_output_tokens if max_output_tokens else capabilities.max_output_tokens,\n        )\n\n        # Add thinking configuration for models that support it\n        # Note: The new API doesn\'t support thinking_config yet\n        # TODO: Update when API adds thinking mode support\n        if capabilities.supports_extended_thinking and thinking_mode in self.THINKING_BUDGETS:\n            logger.debug(f"Thinking mode \'{thinking_mode}\' requested but not yet supported in current API")\n\n        # Retry logic with progressive delays\n        max_retries = 4  # Total of 4 attempts\n        retry_delays = [1, 3, 5, 8]  # Progressive delays: 1s, 3s, 5s, 8s\n\n        last_exception = None\n\n        for attempt in range(max_retries):\n            try:\n                # Ensure API is configured\n                self._ensure_configured()\n\n                # Create model instance\n                model = genai.GenerativeModel(\n                    model_name=resolved_name,\n                    generation_config=generation_config,\n                    system_instruction=system_prompt if system_prompt else None,\n                )\n\n                # Generate content\n                response = model.generate_content(contents)\n\n                # Extract usage information if available\n                usage = self._extract_usage(response)\n\n                return ModelResponse(\n                    content=response.text,\n                    usage=usage,\n                    model_name=resolved_name,\n                    friendly_name="Gemini",\n                    provider=ProviderType.GOOGLE,\n                    metadata={\n                        "thinking_mode": thinking_mode if capabilities.supports_extended_thinking else None,\n                        "finish_reason": (\n                            getattr(response.candidates[0], "finish_reason", "STOP") if response.candidates else "STOP"\n                        ),\n                    },\n                )\n\n            except Exception as e:\n                last_exception = e\n\n                # Check if this is a retryable error using structured error codes\n                is_retryable = self._is_error_retryable(e)\n\n                # If this is the last attempt or not retryable, give up\n                if attempt == max_retries - 1 or not is_retryable:\n                    break\n\n                # Get progressive delay\n                delay = retry_delays[attempt]\n\n                # Log retry attempt\n                logger.warning(\n                    f"Gemini API error for model {resolved_name}, attempt {attempt + 1}/{max_retries}: {str(e)}. Retrying in {delay}s..."\n                )\n                time.sleep(delay)\n\n        # If we get here, all retries failed\n        actual_attempts = attempt + 1  # Convert from 0-based index to human-readable count\n        error_msg = f"Gemini API error for model {resolved_name} after {actual_attempts} attempt{\'s\' if actual_attempts > 1 else \'\'}: {str(last_exception)}"\n        raise RuntimeError(error_msg) from last_exception\n\n    def count_tokens(self, text: str, model_name: str) -> int:\n        """Count tokens for the given text using Gemini\'s tokenizer."""\n        self._resolve_model_name(model_name)\n\n        # For now, use a simple estimation\n        # TODO: Use actual Gemini tokenizer when available in SDK\n        # Rough estimation: ~4 characters per token for English text\n        return len(text) // 4\n\n    def get_provider_type(self) -> ProviderType:\n        """Get the provider type."""\n        return ProviderType.GOOGLE\n\n    def validate_model_name(self, model_name: str) -> bool:\n        """Validate if the model name is supported and allowed."""\n        resolved_name = self._resolve_model_name(model_name)\n\n        # First check if model is supported\n        if resolved_name not in self.SUPPORTED_MODELS:\n            return False\n\n        # Then check if model is allowed by restrictions\n        from utils.model_restrictions import get_restriction_service\n\n        restriction_service = get_restriction_service()\n        # IMPORTANT: Parameter order is (provider_type, model_name, original_name)\n        # resolved_name is the canonical model name, model_name is the user input\n        if not restriction_service.is_allowed(ProviderType.GOOGLE, resolved_name, model_name):\n            logger.debug(f"Gemini model \'{model_name}\' -> \'{resolved_name}\' blocked by restrictions")\n            return False\n\n        return True\n\n    def supports_thinking_mode(self, model_name: str) -> bool:\n        """Check if the model supports extended thinking mode."""\n        capabilities = self.get_capabilities(model_name)\n        return capabilities.supports_extended_thinking\n\n    def get_thinking_budget(self, model_name: str, thinking_mode: str) -> int:\n        """Get actual thinking token budget for a model and thinking mode."""\n        resolved_name = self._resolve_model_name(model_name)\n        model_config = self.SUPPORTED_MODELS.get(resolved_name)\n\n        if not model_config or not model_config.supports_extended_thinking:\n            return 0\n\n        if thinking_mode not in self.THINKING_BUDGETS:\n            return 0\n\n        max_thinking_tokens = model_config.max_thinking_tokens\n        if max_thinking_tokens == 0:\n            return 0\n\n        return int(max_thinking_tokens * self.THINKING_BUDGETS[thinking_mode])\n\n    def _extract_usage(self, response) -> dict[str, int]:\n        """Extract token usage from Gemini response."""\n        usage = {}\n\n        # Try to extract usage metadata from response\n        # Note: The actual structure depends on the SDK version and response format\n        if hasattr(response, "usage_metadata"):\n            metadata = response.usage_metadata\n\n            # Extract token counts with explicit None checks\n            input_tokens = None\n            output_tokens = None\n\n            if hasattr(metadata, "prompt_token_count"):\n                value = metadata.prompt_token_count\n                if value is not None:\n                    input_tokens = value\n                    usage["input_tokens"] = value\n\n            if hasattr(metadata, "candidates_token_count"):\n                value = metadata.candidates_token_count\n                if value is not None:\n                    output_tokens = value\n                    usage["output_tokens"] = value\n\n            # Calculate total only if both values are available and valid\n            if input_tokens is not None and output_tokens is not None:\n                usage["total_tokens"] = input_tokens + output_tokens\n\n        return usage\n\n    def _supports_vision(self, model_name: str) -> bool:\n        """Check if the model supports vision (image processing)."""\n        # Gemini 2.5 models support vision\n        vision_models = {\n            "gemini-2.5-flash",\n            "gemini-2.5-pro",\n            "gemini-2.0-flash",\n            "gemini-1.5-pro",\n            "gemini-1.5-flash",\n        }\n        return model_name in vision_models\n\n    def _is_error_retryable(self, error: Exception) -> bool:\n        """Determine if an error should be retried based on structured error codes.\n\n        Uses Gemini API error structure instead of text pattern matching for reliability.\n\n        Args:\n            error: Exception from Gemini API call\n\n        Returns:\n            True if error should be retried, False otherwise\n        """\n        error_str = str(error).lower()\n\n        # Check for 429 errors first - these need special handling\n        if "429" in error_str or "quota" in error_str or "resource_exhausted" in error_str:\n            # For Gemini, check for specific non-retryable error indicators\n            # These typically indicate permanent failures or quota/size limits\n            non_retryable_indicators = [\n                "quota exceeded",\n                "resource exhausted",\n                "context length",\n                "token limit",\n                "request too large",\n                "invalid request",\n                "quota_exceeded",\n                "resource_exhausted",\n            ]\n\n            # Also check if this is a structured error from Gemini SDK\n            try:\n                # Try to access error details if available\n                if hasattr(error, "details") or hasattr(error, "reason"):\n                    # Gemini API errors may have structured details\n                    error_details = getattr(error, "details", "") or getattr(error, "reason", "")\n                    error_details_str = str(error_details).lower()\n\n                    # Check for non-retryable error codes/reasons\n                    if any(indicator in error_details_str for indicator in non_retryable_indicators):\n                        logger.debug(f"Non-retryable Gemini error: {error_details}")\n                        return False\n            except Exception:\n                pass\n\n            # Check main error string for non-retryable patterns\n            if any(indicator in error_str for indicator in non_retryable_indicators):\n                logger.debug(f"Non-retryable Gemini error based on message: {error_str[:200]}...")\n                return False\n\n            # If it\'s a 429/quota error but doesn\'t match non-retryable patterns, it might be retryable rate limiting\n            logger.debug(f"Retryable Gemini rate limiting error: {error_str[:100]}...")\n            return True\n\n        # For non-429 errors, check if they\'re retryable\n        retryable_indicators = [\n            "timeout",\n            "connection",\n            "network",\n            "temporary",\n            "unavailable",\n            "retry",\n            "internal error",\n            "408",  # Request timeout\n            "500",  # Internal server error\n            "502",  # Bad gateway\n            "503",  # Service unavailable\n            "504",  # Gateway timeout\n            "ssl",  # SSL errors\n            "handshake",  # Handshake failures\n        ]\n\n        return any(indicator in error_str for indicator in retryable_indicators)\n\n    def _process_image(self, image_path: str) -> Optional[dict]:\n        """Process an image for Gemini API."""\n        try:\n            if image_path.startswith("data:image/"):\n                # Handle data URL: data:image/png;base64,iVBORw0...\n                header, data = image_path.split(",", 1)\n                mime_type = header.split(";")[0].split(":")[1]\n                return {"inline_data": {"mime_type": mime_type, "data": data}}\n            else:\n                # Handle file path\n                from utils.file_types import get_image_mime_type\n\n                if not os.path.exists(image_path):\n                    logger.warning(f"Image file not found: {image_path}")\n                    return None\n\n                # Detect MIME type from file extension using centralized mappings\n                ext = os.path.splitext(image_path)[1].lower()\n                mime_type = get_image_mime_type(ext)\n\n                # Read and encode the image\n                with open(image_path, "rb") as f:\n                    image_data = base64.b64encode(f.read()).decode()\n\n                return {"inline_data": {"mime_type": mime_type, "data": image_data}}\n        except Exception as e:\n            logger.error(f"Error processing image {image_path}: {e}")\n            return None\n'
  
  'genai.configure(' is contained here:
    class GeminiModelProvider(ModelProvider):
        """Google Gemini model provider implementation."""
    
        # Model configurations using ModelCapabilities objects
        SUPPORTED_MODELS = {
            "gemini-2.0-flash": ModelCapabilities(
                provider=ProviderType.GOOGLE,
                model_name="gemini-2.0-flash",
                friendly_name="Gemini (Flash 2.0)",
                context_window=1_048_576,  # 1M tokens
                max_output_tokens=65_536,
                supports_extended_thinking=True,  # Experimental thinking mode
                supports_system_prompts=True,
                supports_streaming=True,
                supports_function_calling=True,
                supports_json_mode=True,
                supports_images=True,  # Vision capability
                max_image_size_mb=20.0,  # Conservative 20MB limit for reliability
                supports_temperature=True,
                temperature_constraint=create_temperature_constraint("range"),
                max_thinking_tokens=24576,  # Same as 2.5 flash for consistency
                description="Gemini 2.0 Flash (1M context) - Latest fast model with experimental thinking, supports audio/video input",
                aliases=["flash-2.0", "flash2"],
            ),
            "gemini-2.0-flash-lite": ModelCapabilities(
                provider=ProviderType.GOOGLE,
                model_name="gemini-2.0-flash-lite",
                friendly_name="Gemin (Flash Lite 2.0)",
                context_window=1_048_576,  # 1M tokens
                max_output_tokens=65_536,
                supports_extended_thinking=False,  # Not supported per user request
                supports_system_prompts=True,
                supports_streaming=True,
                supports_function_calling=True,
                supports_json_mode=True,
                supports_images=False,  # Does not support images
                max_image_size_mb=0.0,  # No image support
                supports_temperature=True,
                temperature_constraint=create_temperature_constraint("range"),
                description="Gemini 2.0 Flash Lite (1M context) - Lightweight fast model, text-only",
                aliases=["flashlite", "flash-lite"],
            ),
            "gemini-2.5-flash": ModelCapabilities(
                provider=ProviderType.GOOGLE,
                model_name="gemini-2.5-flash",
                friendly_name="Gemini (Flash 2.5)",
                context_window=1_048_576,  # 1M tokens
                max_output_tokens=65_536,
                supports_extended_thinking=True,
                supports_system_prompts=True,
                supports_streaming=True,
                supports_function_calling=True,
                supports_json_mode=True,
                supports_images=True,  # Vision capability
                max_image_size_mb=20.0,  # Conservative 20MB limit for reliability
                supports_temperature=True,
                temperature_constraint=create_temperature_constraint("range"),
                max_thinking_tokens=24576,  # Flash 2.5 thinking budget limit
                description="Ultra-fast (1M context) - Quick analysis, simple queries, rapid iterations",
                aliases=["flash", "flash2.5"],
            ),
            "gemini-2.5-pro": ModelCapabilities(
                provider=ProviderType.GOOGLE,
                model_name="gemini-2.5-pro",
                friendly_name="Gemini (Pro 2.5)",
                context_window=1_048_576,  # 1M tokens
                max_output_tokens=65_536,
                supports_extended_thinking=True,
                supports_system_prompts=True,
                supports_streaming=True,
                supports_function_calling=True,
                supports_json_mode=True,
                supports_images=True,  # Vision capability
                max_image_size_mb=32.0,  # Higher limit for Pro model
                supports_temperature=True,
                temperature_constraint=create_temperature_constraint("range"),
                max_thinking_tokens=32768,  # Max thinking tokens for Pro model
                description="Deep reasoning + thinking mode (1M context) - Complex problems, architecture, deep analysis",
                aliases=["pro", "gemini pro", "gemini-pro"],
            ),
        }
    
        # Thinking mode configurations - percentages of model's max_thinking_tokens
        # These percentages work across all models that support thinking
        THINKING_BUDGETS = {
            "minimal": 0.005,  # 0.5% of max - minimal thinking for fast responses
            "low": 0.08,  # 8% of max - light reasoning tasks
            "medium": 0.33,  # 33% of max - balanced reasoning (default)
            "high": 0.67,  # 67% of max - complex analysis
            "max": 1.0,  # 100% of max - full thinking budget
        }
    
        # Model-specific thinking token limits
        MAX_THINKING_TOKENS = {
            "gemini-2.0-flash": 24576,  # Same as 2.5 flash for consistency
            "gemini-2.0-flash-lite": 0,  # No thinking support
            "gemini-2.5-flash": 24576,  # Flash 2.5 thinking budget limit
            "gemini-2.5-pro": 32768,  # Pro 2.5 thinking budget limit
        }
    
        def __init__(self, api_key: str, **kwargs):
            """Initialize Gemini provider with API key."""
            super().__init__(api_key, **kwargs)
            self._configured = False
            self._token_counters = {}  # Cache for token counting
    
        def _ensure_configured(self):
            """Ensure Gemini API is configured."""
            if not self._configured:
                genai.configure(api_key=self.api_key)
  ?             ++++++++++++++++
                self._configured = True
    
        def get_capabilities(self, model_name: str) -> ModelCapabilities:
            """Get capabilities for a specific Gemini model."""
            # Resolve shorthand
            resolved_name = self._resolve_model_name(model_name)
    
            if resolved_name not in self.SUPPORTED_MODELS:
                raise ValueError(f"Unsupported Gemini model: {model_name}")
    
            # Check if model is allowed by restrictions
            from utils.model_restrictions import get_restriction_service
    
            restriction_service = get_restriction_service()
            # IMPORTANT: Parameter order is (provider_type, model_name, original_name)
            # resolved_name is the canonical model name, model_name is the user input
            if not restriction_service.is_allowed(ProviderType.GOOGLE, resolved_name, model_name):
                raise ValueError(f"Gemini model '{resolved_name}' is not allowed by restriction policy.")
    
            # Return the ModelCapabilities object directly from SUPPORTED_MODELS
            return self.SUPPORTED_MODELS[resolved_name]
    
        def generate_content(
            self,
            prompt: str,
            model_name: str,
            system_prompt: Optional[str] = None,
            temperature: float = 0.7,
            max_output_tokens: Optional[int] = None,
            thinking_mode: str = "medium",
            images: Optional[list[str]] = None,
            **kwargs,
        ) -> ModelResponse:
            """Generate content using Gemini model."""
            # Validate parameters
            resolved_name = self._resolve_model_name(model_name)
            self.validate_parameters(model_name, temperature)
    
            # Prepare content parts (text and potentially images)
            parts = []
    
            # Add system and user prompts as text
            if system_prompt:
                full_prompt = f"{system_prompt}\n\n{prompt}"
            else:
                full_prompt = prompt
    
            parts.append({"text": full_prompt})
    
            # Add images if provided and model supports vision
            if images and self._supports_vision(resolved_name):
                for image_path in images:
                    try:
                        image_part = self._process_image(image_path)
                        if image_part:
                            parts.append(image_part)
                    except Exception as e:
                        logger.warning(f"Failed to process image {image_path}: {e}")
                        # Continue with other images and text
                        continue
            elif images and not self._supports_vision(resolved_name):
                logger.warning(f"Model {resolved_name} does not support images, ignoring {len(images)} image(s)")
    
            # Create contents structure
            contents = [{"parts": parts}]
    
            # Get capabilities first
            capabilities = self.get_capabilities(model_name)
    
            # Prepare generation config
            generation_config = genai.GenerationConfig(
                temperature=temperature,
                candidate_count=1,
                max_output_tokens=max_output_tokens if max_output_tokens else capabilities.max_output_tokens,
            )
    
            # Add thinking configuration for models that support it
            # Note: The new API doesn't support thinking_config yet
            # TODO: Update when API adds thinking mode support
            if capabilities.supports_extended_thinking and thinking_mode in self.THINKING_BUDGETS:
                logger.debug(f"Thinking mode '{thinking_mode}' requested but not yet supported in current API")
    
            # Retry logic with progressive delays
            max_retries = 4  # Total of 4 attempts
            retry_delays = [1, 3, 5, 8]  # Progressive delays: 1s, 3s, 5s, 8s
    
            last_exception = None
    
            for attempt in range(max_retries):
                try:
                    # Ensure API is configured
                    self._ensure_configured()
    
                    # Create model instance
                    model = genai.GenerativeModel(
                        model_name=resolved_name,
                        generation_config=generation_config,
                        system_instruction=system_prompt if system_prompt else None,
                    )
    
                    # Generate content
                    response = model.generate_content(contents)
    
                    # Extract usage information if available
                    usage = self._extract_usage(response)
    
                    return ModelResponse(
                        content=response.text,
                        usage=usage,
                        model_name=resolved_name,
                        friendly_name="Gemini",
                        provider=ProviderType.GOOGLE,
                        metadata={
                            "thinking_mode": thinking_mode if capabilities.supports_extended_thinking else None,
                            "finish_reason": (
                                getattr(response.candidates[0], "finish_reason", "STOP") if response.candidates else "STOP"
                            ),
                        },
                    )
    
                except Exception as e:
                    last_exception = e
    
                    # Check if this is a retryable error using structured error codes
                    is_retryable = self._is_error_retryable(e)
    
                    # If this is the last attempt or not retryable, give up
                    if attempt == max_retries - 1 or not is_retryable:
                        break
    
                    # Get progressive delay
                    delay = retry_delays[attempt]
    
                    # Log retry attempt
                    logger.warning(
                        f"Gemini API error for model {resolved_name}, attempt {attempt + 1}/{max_retries}: {str(e)}. Retrying in {delay}s..."
                    )
                    time.sleep(delay)
    
            # If we get here, all retries failed
            actual_attempts = attempt + 1  # Convert from 0-based index to human-readable count
            error_msg = f"Gemini API error for model {resolved_name} after {actual_attempts} attempt{'s' if actual_attempts > 1 else ''}: {str(last_exception)}"
            raise RuntimeError(error_msg) from last_exception
    
        def count_tokens(self, text: str, model_name: str) -> int:
            """Count tokens for the given text using Gemini's tokenizer."""
            self._resolve_model_name(model_name)
    
            # For now, use a simple estimation
            # TODO: Use actual Gemini tokenizer when available in SDK
            # Rough estimation: ~4 characters per token for English text
            return len(text) // 4
    
        def get_provider_type(self) -> ProviderType:
            """Get the provider type."""
            return ProviderType.GOOGLE
    
        def validate_model_name(self, model_name: str) -> bool:
            """Validate if the model name is supported and allowed."""
            resolved_name = self._resolve_model_name(model_name)
    
            # First check if model is supported
            if resolved_name not in self.SUPPORTED_MODELS:
                return False
    
            # Then check if model is allowed by restrictions
            from utils.model_restrictions import get_restriction_service
    
            restriction_service = get_restriction_service()
            # IMPORTANT: Parameter order is (provider_type, model_name, original_name)
            # resolved_name is the canonical model name, model_name is the user input
            if not restriction_service.is_allowed(ProviderType.GOOGLE, resolved_name, model_name):
                logger.debug(f"Gemini model '{model_name}' -> '{resolved_name}' blocked by restrictions")
                return False
    
            return True
    
        def supports_thinking_mode(self, model_name: str) -> bool:
            """Check if the model supports extended thinking mode."""
            capabilities = self.get_capabilities(model_name)
            return capabilities.supports_extended_thinking
    
        def get_thinking_budget(self, model_name: str, thinking_mode: str) -> int:
            """Get actual thinking token budget for a model and thinking mode."""
            resolved_name = self._resolve_model_name(model_name)
            model_config = self.SUPPORTED_MODELS.get(resolved_name)
    
            if not model_config or not model_config.supports_extended_thinking:
                return 0
    
            if thinking_mode not in self.THINKING_BUDGETS:
                return 0
    
            max_thinking_tokens = model_config.max_thinking_tokens
            if max_thinking_tokens == 0:
                return 0
    
            return int(max_thinking_tokens * self.THINKING_BUDGETS[thinking_mode])
    
        def _extract_usage(self, response) -> dict[str, int]:
            """Extract token usage from Gemini response."""
            usage = {}
    
            # Try to extract usage metadata from response
            # Note: The actual structure depends on the SDK version and response format
            if hasattr(response, "usage_metadata"):
                metadata = response.usage_metadata
    
                # Extract token counts with explicit None checks
                input_tokens = None
                output_tokens = None
    
                if hasattr(metadata, "prompt_token_count"):
                    value = metadata.prompt_token_count
                    if value is not None:
                        input_tokens = value
                        usage["input_tokens"] = value
    
                if hasattr(metadata, "candidates_token_count"):
                    value = metadata.candidates_token_count
                    if value is not None:
                        output_tokens = value
                        usage["output_tokens"] = value
    
                # Calculate total only if both values are available and valid
                if input_tokens is not None and output_tokens is not None:
                    usage["total_tokens"] = input_tokens + output_tokens
    
            return usage
    
        def _supports_vision(self, model_name: str) -> bool:
            """Check if the model supports vision (image processing)."""
            # Gemini 2.5 models support vision
            vision_models = {
                "gemini-2.5-flash",
                "gemini-2.5-pro",
                "gemini-2.0-flash",
                "gemini-1.5-pro",
                "gemini-1.5-flash",
            }
            return model_name in vision_models
    
        def _is_error_retryable(self, error: Exception) -> bool:
            """Determine if an error should be retried based on structured error codes.
    
            Uses Gemini API error structure instead of text pattern matching for reliability.
    
            Args:
                error: Exception from Gemini API call
    
            Returns:
                True if error should be retried, False otherwise
            """
            error_str = str(error).lower()
    
            # Check for 429 errors first - these need special handling
            if "429" in error_str or "quota" in error_str or "resource_exhausted" in error_str:
                # For Gemini, check for specific non-retryable error indicators
                # These typically indicate permanent failures or quota/size limits
                non_retryable_indicators = [
                    "quota exceeded",
                    "resource exhausted",
                    "context length",
                    "token limit",
                    "request too large",
                    "invalid request",
                    "quota_exceeded",
                    "resource_exhausted",
                ]
    
                # Also check if this is a structured error from Gemini SDK
                try:
                    # Try to access error details if available
                    if hasattr(error, "details") or hasattr(error, "reason"):
                        # Gemini API errors may have structured details
                        error_details = getattr(error, "details", "") or getattr(error, "reason", "")
                        error_details_str = str(error_details).lower()
    
                        # Check for non-retryable error codes/reasons
                        if any(indicator in error_details_str for indicator in non_retryable_indicators):
                            logger.debug(f"Non-retryable Gemini error: {error_details}")
                            return False
                except Exception:
                    pass
    
                # Check main error string for non-retryable patterns
                if any(indicator in error_str for indicator in non_retryable_indicators):
                    logger.debug(f"Non-retryable Gemini error based on message: {error_str[:200]}...")
                    return False
    
                # If it's a 429/quota error but doesn't match non-retryable patterns, it might be retryable rate limiting
                logger.debug(f"Retryable Gemini rate limiting error: {error_str[:100]}...")
                return True
    
            # For non-429 errors, check if they're retryable
            retryable_indicators = [
                "timeout",
                "connection",
                "network",
                "temporary",
                "unavailable",
                "retry",
                "internal error",
                "408",  # Request timeout
                "500",  # Internal server error
                "502",  # Bad gateway
                "503",  # Service unavailable
                "504",  # Gateway timeout
                "ssl",  # SSL errors
                "handshake",  # Handshake failures
            ]
    
            return any(indicator in error_str for indicator in retryable_indicators)
    
        def _process_image(self, image_path: str) -> Optional[dict]:
            """Process an image for Gemini API."""
            try:
                if image_path.startswith("data:image/"):
                    # Handle data URL: data:image/png;base64,iVBORw0...
                    header, data = image_path.split(",", 1)
                    mime_type = header.split(";")[0].split(":")[1]
                    return {"inline_data": {"mime_type": mime_type, "data": data}}
                else:
                    # Handle file path
                    from utils.file_types import get_image_mime_type
    
                    if not os.path.exists(image_path):
                        logger.warning(f"Image file not found: {image_path}")
                        return None
    
                    # Detect MIME type from file extension using centralized mappings
                    ext = os.path.splitext(image_path)[1].lower()
                    mime_type = get_image_mime_type(ext)
    
                    # Read and encode the image
                    with open(image_path, "rb") as f:
                        image_data = base64.b64encode(f.read()).decode()
    
                    return {"inline_data": {"mime_type": mime_type, "data": image_data}}
            except Exception as e:
                logger.error(f"Error processing image {image_path}: {e}")
                return None
FAILED tests/test_gemini_provider.py::test_gemini_provider_uses_client_pattern - AssertionError: Should use new Client() pattern for SDK initialization
assert ('genai.Client(' in 'class GeminiModelProvider(ModelProvider):\n    """Google Gemini model provider implementation."""\n\n    # Model configurations using ModelCapabilities objects\n    SUPPORTED_MODELS = {\n        "gemini-2.0-flash": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.0-flash",\n            friendly_name="Gemini (Flash 2.0)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,  # Experimental thinking mode\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=20.0,  # Conservative 20MB limit for reliability\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=24576,  # Same as 2.5 flash for consistency\n            description="Gemini 2.0 Flash (1M context) - Latest fast model with experimental thinking, supports audio/video input",\n            aliases=["flash-2.0", "flash2"],\n        ),\n        "gemini-2.0-flash-lite": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.0-flash-lite",\n            friendly_name="Gemin (Flash Lite 2.0)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=False,  # Not supported per user request\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=False,  # Does not support images\n            max_image_size_mb=0.0,  # No image support\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            description="Gemini 2.0 Flash Lite (1M context) - Lightweight fast model, text-only",\n            aliases=["flashlite", "flash-lite"],\n        ),\n        "gemini-2.5-flash": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.5-flash",\n            friendly_name="Gemini (Flash 2.5)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=20.0,  # Conservative 20MB limit for reliability\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=24576,  # Flash 2.5 thinking budget limit\n            description="Ultra-fast (1M context) - Quick analysis, simple queries, rapid iterations",\n            aliases=["flash", "flash2.5"],\n        ),\n        "gemini-2.5-pro": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.5-pro",\n            friendly_name="Gemini (Pro 2.5)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=32.0,  # Higher limit for Pro model\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=32768,  # Max thinking tokens for Pro model\n            description="Deep reasoning + thinking mode (1M context) - Complex problems, architecture, deep analysis",\n            aliases=["pro", "gemini pro", "gemini-pro"],\n        ),\n    }\n\n    # Thinking mode configurations - percentages of model\'s max_thinking_tokens\n    # These percentages work across all models that support thinking\n    THINKING_BUDGETS = {\n        "minimal": 0.005,  # 0.5% of max - minimal thinking for fast responses\n        "low": 0.08,  # 8% of max - light reasoning tasks\n        "medium": 0.33,  # 33% of max - balanced reasoning (default)\n        "high": 0.67,  # 67% of max - complex analysis\n        "max": 1.0,  # 100% of max - full thinking budget\n    }\n\n    # Model-specific thinking token limits\n    MAX_THINKING_TOKENS = {\n        "gemini-2.0-flash": 24576,  # Same as 2.5 flash for consistency\n        "gemini-2.0-flash-lite": 0,  # No thinking support\n        "gemini-2.5-flash": 24576,  # Flash 2.5 thinking budget limit\n        "gemini-2.5-pro": 32768,  # Pro 2.5 thinking budget limit\n    }\n\n    def __init__(self, api_key: str, **kwargs):\n        """Initialize Gemini provider with API key."""\n        super().__init__(api_key, **kwargs)\n        self._configured = False\n        self._token_counters = {}  # Cache for token counting\n\n    def _ensure_configured(self):\n        """Ensure Gemini API is configured."""\n        if not self._configured:\n            genai.configure(api_key=self.api_key)\n            self._configured = True\n\n    def get_capabilities(self, model_name: str) -> ModelCapabilities:\n        """Get capabilities for a specific Gemini model."""\n        # Resolve shorthand\n        resolved_name = self._resolve_model_name(model_name)\n\n        if resolved_name not in self.SUPPORTED_MODELS:\n            raise ValueError(f"Unsupported Gemini model: {model_name}")\n\n        # Check if model is allowed by restrictions\n        from utils.model_restrictions import get_restriction_service\n\n        restriction_service = get_restriction_service()\n        # IMPORTANT: Parameter order is (provider_type, model_name, original_name)\n        # resolved_name is the canonical model name, model_name is the user input\n        if not restriction_service.is_allowed(ProviderType.GOOGLE, resolved_name, model_name):\n            raise ValueError(f"Gemini model \'{resolved_name}\' is not allowed by restriction policy.")\n\n        # Return the ModelCapabilities object directly from SUPPORTED_MODELS\n        return self.SUPPORTED_MODELS[resolved_name]\n\n    def generate_content(\n        self,\n        prompt: str,\n        model_name: str,\n        system_prompt: Optional[str] = None,\n        temperature: float = 0.7,\n        max_output_tokens: Optional[int] = None,\n        thinking_mode: str = "medium",\n        images: Optional[list[str]] = None,\n        **kwargs,\n    ) -> ModelResponse:\n        """Generate content using Gemini model."""\n        # Validate parameters\n        resolved_name = self._resolve_model_name(model_name)\n        self.validate_parameters(model_name, temperature)\n\n        # Prepare content parts (text and potentially images)\n        parts = []\n\n        # Add system and user prompts as text\n        if system_prompt:\n            full_prompt = f"{system_prompt}\\n\\n{prompt}"\n        else:\n            full_prompt = prompt\n\n        parts.append({"text": full_prompt})\n\n        # Add images if provided and model supports vision\n        if images and self._supports_vision(resolved_name):\n            for image_path in images:\n                try:\n                    image_part = self._process_image(image_path)\n                    if image_part:\n                        parts.append(image_part)\n                except Exception as e:\n                    logger.warning(f"Failed to process image {image_path}: {e}")\n                    # Continue with other images and text\n                    continue\n        elif images and not self._supports_vision(resolved_name):\n            logger.warning(f"Model {resolved_name} does not support images, ignoring {len(images)} image(s)")\n\n        # Create contents structure\n        contents = [{"parts": parts}]\n\n        # Get capabilities first\n        capabilities = self.get_capabilities(model_name)\n\n        # Prepare generation config\n        generation_config = genai.GenerationConfig(\n            temperature=temperature,\n            candidate_count=1,\n            max_output_tokens=max_output_tokens if max_output_tokens else capabilities.max_output_tokens,\n        )\n\n        # Add thinking configuration for models that support it\n        # Note: The new API doesn\'t support thinking_config yet\n        # TODO: Update when API adds thinking mode support\n        if capabilities.supports_extended_thinking and thinking_mode in self.THINKING_BUDGETS:\n            logger.debug(f"Thinking mode \'{thinking_mode}\' requested but not yet supported in current API")\n\n        # Retry logic with progressive delays\n        max_retries = 4  # Total of 4 attempts\n        retry_delays = [1, 3, 5, 8]  # Progressive delays: 1s, 3s, 5s, 8s\n\n        last_exception = None\n\n        for attempt in range(max_retries):\n            try:\n                # Ensure API is configured\n                self._ensure_configured()\n\n                # Create model instance\n                model = genai.GenerativeModel(\n                    model_name=resolved_name,\n                    generation_config=generation_config,\n                    system_instruction=system_prompt if system_prompt else None,\n                )\n\n                # Generate content\n                response = model.generate_content(contents)\n\n                # Extract usage information if available\n                usage = self._extract_usage(response)\n\n                return ModelResponse(\n                    content=response.text,\n                    usage=usage,\n                    model_name=resolved_name,\n                    friendly_name="Gemini",\n                    provider=ProviderType.GOOGLE,\n                    metadata={\n                        "thinking_mode": thinking_mode if capabilities.supports_extended_thinking else None,\n                        "finish_reason": (\n                            getattr(response.candidates[0], "finish_reason", "STOP") if response.candidates else "STOP"\n                        ),\n                    },\n                )\n\n            except Exception as e:\n                last_exception = e\n\n                # Check if this is a retryable error using structured error codes\n                is_retryable = self._is_error_retryable(e)\n\n                # If this is the last attempt or not retryable, give up\n                if attempt == max_retries - 1 or not is_retryable:\n                    break\n\n                # Get progressive delay\n                delay = retry_delays[attempt]\n\n                # Log retry attempt\n                logger.warning(\n                    f"Gemini API error for model {resolved_name}, attempt {attempt + 1}/{max_retries}: {str(e)}. Retrying in {delay}s..."\n                )\n                time.sleep(delay)\n\n        # If we get here, all retries failed\n        actual_attempts = attempt + 1  # Convert from 0-based index to human-readable count\n        error_msg = f"Gemini API error for model {resolved_name} after {actual_attempts} attempt{\'s\' if actual_attempts > 1 else \'\'}: {str(last_exception)}"\n        raise RuntimeError(error_msg) from last_exception\n\n    def count_tokens(self, text: str, model_name: str) -> int:\n        """Count tokens for the given text using Gemini\'s tokenizer."""\n        self._resolve_model_name(model_name)\n\n        # For now, use a simple estimation\n        # TODO: Use actual Gemini tokenizer when available in SDK\n        # Rough estimation: ~4 characters per token for English text\n        return len(text) // 4\n\n    def get_provider_type(self) -> ProviderType:\n        """Get the provider type."""\n        return ProviderType.GOOGLE\n\n    def validate_model_name(self, model_name: str) -> bool:\n        """Validate if the model name is supported and allowed."""\n        resolved_name = self._resolve_model_name(model_name)\n\n        # First check if model is supported\n        if resolved_name not in self.SUPPORTED_MODELS:\n            return False\n\n        # Then check if model is allowed by restrictions\n        from utils.model_restrictions import get_restriction_service\n\n        restriction_service = get_restriction_service()\n        # IMPORTANT: Parameter order is (provider_type, model_name, original_name)\n        # resolved_name is the canonical model name, model_name is the user input\n        if not restriction_service.is_allowed(ProviderType.GOOGLE, resolved_name, model_name):\n            logger.debug(f"Gemini model \'{model_name}\' -> \'{resolved_name}\' blocked by restrictions")\n            return False\n\n        return True\n\n    def supports_thinking_mode(self, model_name: str) -> bool:\n        """Check if the model supports extended thinking mode."""\n        capabilities = self.get_capabilities(model_name)\n        return capabilities.supports_extended_thinking\n\n    def get_thinking_budget(self, model_name: str, thinking_mode: str) -> int:\n        """Get actual thinking token budget for a model and thinking mode."""\n        resolved_name = self._resolve_model_name(model_name)\n        model_config = self.SUPPORTED_MODELS.get(resolved_name)\n\n        if not model_config or not model_config.supports_extended_thinking:\n            return 0\n\n        if thinking_mode not in self.THINKING_BUDGETS:\n            return 0\n\n        max_thinking_tokens = model_config.max_thinking_tokens\n        if max_thinking_tokens == 0:\n            return 0\n\n        return int(max_thinking_tokens * self.THINKING_BUDGETS[thinking_mode])\n\n    def _extract_usage(self, response) -> dict[str, int]:\n        """Extract token usage from Gemini response."""\n        usage = {}\n\n        # Try to extract usage metadata from response\n        # Note: The actual structure depends on the SDK version and response format\n        if hasattr(response, "usage_metadata"):\n            metadata = response.usage_metadata\n\n            # Extract token counts with explicit None checks\n            input_tokens = None\n            output_tokens = None\n\n            if hasattr(metadata, "prompt_token_count"):\n                value = metadata.prompt_token_count\n                if value is not None:\n                    input_tokens = value\n                    usage["input_tokens"] = value\n\n            if hasattr(metadata, "candidates_token_count"):\n                value = metadata.candidates_token_count\n                if value is not None:\n                    output_tokens = value\n                    usage["output_tokens"] = value\n\n            # Calculate total only if both values are available and valid\n            if input_tokens is not None and output_tokens is not None:\n                usage["total_tokens"] = input_tokens + output_tokens\n\n        return usage\n\n    def _supports_vision(self, model_name: str) -> bool:\n        """Check if the model supports vision (image processing)."""\n        # Gemini 2.5 models support vision\n        vision_models = {\n            "gemini-2.5-flash",\n            "gemini-2.5-pro",\n            "gemini-2.0-flash",\n            "gemini-1.5-pro",\n            "gemini-1.5-flash",\n        }\n        return model_name in vision_models\n\n    def _is_error_retryable(self, error: Exception) -> bool:\n        """Determine if an error should be retried based on structured error codes.\n\n        Uses Gemini API error structure instead of text pattern matching for reliability.\n\n        Args:\n            error: Exception from Gemini API call\n\n        Returns:\n            True if error should be retried, False otherwise\n        """\n        error_str = str(error).lower()\n\n        # Check for 429 errors first - these need special handling\n        if "429" in error_str or "quota" in error_str or "resource_exhausted" in error_str:\n            # For Gemini, check for specific non-retryable error indicators\n            # These typically indicate permanent failures or quota/size limits\n            non_retryable_indicators = [\n                "quota exceeded",\n                "resource exhausted",\n                "context length",\n                "token limit",\n                "request too large",\n                "invalid request",\n                "quota_exceeded",\n                "resource_exhausted",\n            ]\n\n            # Also check if this is a structured error from Gemini SDK\n            try:\n                # Try to access error details if available\n                if hasattr(error, "details") or hasattr(error, "reason"):\n                    # Gemini API errors may have structured details\n                    error_details = getattr(error, "details", "") or getattr(error, "reason", "")\n                    error_details_str = str(error_details).lower()\n\n                    # Check for non-retryable error codes/reasons\n                    if any(indicator in error_details_str for indicator in non_retryable_indicators):\n                        logger.debug(f"Non-retryable Gemini error: {error_details}")\n                        return False\n            except Exception:\n                pass\n\n            # Check main error string for non-retryable patterns\n            if any(indicator in error_str for indicator in non_retryable_indicators):\n                logger.debug(f"Non-retryable Gemini error based on message: {error_str[:200]}...")\n                return False\n\n            # If it\'s a 429/quota error but doesn\'t match non-retryable patterns, it might be retryable rate limiting\n            logger.debug(f"Retryable Gemini rate limiting error: {error_str[:100]}...")\n            return True\n\n        # For non-429 errors, check if they\'re retryable\n        retryable_indicators = [\n            "timeout",\n            "connection",\n            "network",\n            "temporary",\n            "unavailable",\n            "retry",\n            "internal error",\n            "408",  # Request timeout\n            "500",  # Internal server error\n            "502",  # Bad gateway\n            "503",  # Service unavailable\n            "504",  # Gateway timeout\n            "ssl",  # SSL errors\n            "handshake",  # Handshake failures\n        ]\n\n        return any(indicator in error_str for indicator in retryable_indicators)\n\n    def _process_image(self, image_path: str) -> Optional[dict]:\n        """Process an image for Gemini API."""\n        try:\n            if image_path.startswith("data:image/"):\n                # Handle data URL: data:image/png;base64,iVBORw0...\n                header, data = image_path.split(",", 1)\n                mime_type = header.split(";")[0].split(":")[1]\n                return {"inline_data": {"mime_type": mime_type, "data": data}}\n            else:\n                # Handle file path\n                from utils.file_types import get_image_mime_type\n\n                if not os.path.exists(image_path):\n                    logger.warning(f"Image file not found: {image_path}")\n                    return None\n\n                # Detect MIME type from file extension using centralized mappings\n                ext = os.path.splitext(image_path)[1].lower()\n                mime_type = get_image_mime_type(ext)\n\n                # Read and encode the image\n                with open(image_path, "rb") as f:\n                    image_data = base64.b64encode(f.read()).decode()\n\n                return {"inline_data": {"mime_type": mime_type, "data": image_data}}\n        except Exception as e:\n            logger.error(f"Error processing image {image_path}: {e}")\n            return None\n' or 'Client(' in 'class GeminiModelProvider(ModelProvider):\n    """Google Gemini model provider implementation."""\n\n    # Model configurations using ModelCapabilities objects\n    SUPPORTED_MODELS = {\n        "gemini-2.0-flash": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.0-flash",\n            friendly_name="Gemini (Flash 2.0)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,  # Experimental thinking mode\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=20.0,  # Conservative 20MB limit for reliability\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=24576,  # Same as 2.5 flash for consistency\n            description="Gemini 2.0 Flash (1M context) - Latest fast model with experimental thinking, supports audio/video input",\n            aliases=["flash-2.0", "flash2"],\n        ),\n        "gemini-2.0-flash-lite": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.0-flash-lite",\n            friendly_name="Gemin (Flash Lite 2.0)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=False,  # Not supported per user request\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=False,  # Does not support images\n            max_image_size_mb=0.0,  # No image support\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            description="Gemini 2.0 Flash Lite (1M context) - Lightweight fast model, text-only",\n            aliases=["flashlite", "flash-lite"],\n        ),\n        "gemini-2.5-flash": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.5-flash",\n            friendly_name="Gemini (Flash 2.5)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=20.0,  # Conservative 20MB limit for reliability\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=24576,  # Flash 2.5 thinking budget limit\n            description="Ultra-fast (1M context) - Quick analysis, simple queries, rapid iterations",\n            aliases=["flash", "flash2.5"],\n        ),\n        "gemini-2.5-pro": ModelCapabilities(\n            provider=ProviderType.GOOGLE,\n            model_name="gemini-2.5-pro",\n            friendly_name="Gemini (Pro 2.5)",\n            context_window=1_048_576,  # 1M tokens\n            max_output_tokens=65_536,\n            supports_extended_thinking=True,\n            supports_system_prompts=True,\n            supports_streaming=True,\n            supports_function_calling=True,\n            supports_json_mode=True,\n            supports_images=True,  # Vision capability\n            max_image_size_mb=32.0,  # Higher limit for Pro model\n            supports_temperature=True,\n            temperature_constraint=create_temperature_constraint("range"),\n            max_thinking_tokens=32768,  # Max thinking tokens for Pro model\n            description="Deep reasoning + thinking mode (1M context) - Complex problems, architecture, deep analysis",\n            aliases=["pro", "gemini pro", "gemini-pro"],\n        ),\n    }\n\n    # Thinking mode configurations - percentages of model\'s max_thinking_tokens\n    # These percentages work across all models that support thinking\n    THINKING_BUDGETS = {\n        "minimal": 0.005,  # 0.5% of max - minimal thinking for fast responses\n        "low": 0.08,  # 8% of max - light reasoning tasks\n        "medium": 0.33,  # 33% of max - balanced reasoning (default)\n        "high": 0.67,  # 67% of max - complex analysis\n        "max": 1.0,  # 100% of max - full thinking budget\n    }\n\n    # Model-specific thinking token limits\n    MAX_THINKING_TOKENS = {\n        "gemini-2.0-flash": 24576,  # Same as 2.5 flash for consistency\n        "gemini-2.0-flash-lite": 0,  # No thinking support\n        "gemini-2.5-flash": 24576,  # Flash 2.5 thinking budget limit\n        "gemini-2.5-pro": 32768,  # Pro 2.5 thinking budget limit\n    }\n\n    def __init__(self, api_key: str, **kwargs):\n        """Initialize Gemini provider with API key."""\n        super().__init__(api_key, **kwargs)\n        self._configured = False\n        self._token_counters = {}  # Cache for token counting\n\n    def _ensure_configured(self):\n        """Ensure Gemini API is configured."""\n        if not self._configured:\n            genai.configure(api_key=self.api_key)\n            self._configured = True\n\n    def get_capabilities(self, model_name: str) -> ModelCapabilities:\n        """Get capabilities for a specific Gemini model."""\n        # Resolve shorthand\n        resolved_name = self._resolve_model_name(model_name)\n\n        if resolved_name not in self.SUPPORTED_MODELS:\n            raise ValueError(f"Unsupported Gemini model: {model_name}")\n\n        # Check if model is allowed by restrictions\n        from utils.model_restrictions import get_restriction_service\n\n        restriction_service = get_restriction_service()\n        # IMPORTANT: Parameter order is (provider_type, model_name, original_name)\n        # resolved_name is the canonical model name, model_name is the user input\n        if not restriction_service.is_allowed(ProviderType.GOOGLE, resolved_name, model_name):\n            raise ValueError(f"Gemini model \'{resolved_name}\' is not allowed by restriction policy.")\n\n        # Return the ModelCapabilities object directly from SUPPORTED_MODELS\n        return self.SUPPORTED_MODELS[resolved_name]\n\n    def generate_content(\n        self,\n        prompt: str,\n        model_name: str,\n        system_prompt: Optional[str] = None,\n        temperature: float = 0.7,\n        max_output_tokens: Optional[int] = None,\n        thinking_mode: str = "medium",\n        images: Optional[list[str]] = None,\n        **kwargs,\n    ) -> ModelResponse:\n        """Generate content using Gemini model."""\n        # Validate parameters\n        resolved_name = self._resolve_model_name(model_name)\n        self.validate_parameters(model_name, temperature)\n\n        # Prepare content parts (text and potentially images)\n        parts = []\n\n        # Add system and user prompts as text\n        if system_prompt:\n            full_prompt = f"{system_prompt}\\n\\n{prompt}"\n        else:\n            full_prompt = prompt\n\n        parts.append({"text": full_prompt})\n\n        # Add images if provided and model supports vision\n        if images and self._supports_vision(resolved_name):\n            for image_path in images:\n                try:\n                    image_part = self._process_image(image_path)\n                    if image_part:\n                        parts.append(image_part)\n                except Exception as e:\n                    logger.warning(f"Failed to process image {image_path}: {e}")\n                    # Continue with other images and text\n                    continue\n        elif images and not self._supports_vision(resolved_name):\n            logger.warning(f"Model {resolved_name} does not support images, ignoring {len(images)} image(s)")\n\n        # Create contents structure\n        contents = [{"parts": parts}]\n\n        # Get capabilities first\n        capabilities = self.get_capabilities(model_name)\n\n        # Prepare generation config\n        generation_config = genai.GenerationConfig(\n            temperature=temperature,\n            candidate_count=1,\n            max_output_tokens=max_output_tokens if max_output_tokens else capabilities.max_output_tokens,\n        )\n\n        # Add thinking configuration for models that support it\n        # Note: The new API doesn\'t support thinking_config yet\n        # TODO: Update when API adds thinking mode support\n        if capabilities.supports_extended_thinking and thinking_mode in self.THINKING_BUDGETS:\n            logger.debug(f"Thinking mode \'{thinking_mode}\' requested but not yet supported in current API")\n\n        # Retry logic with progressive delays\n        max_retries = 4  # Total of 4 attempts\n        retry_delays = [1, 3, 5, 8]  # Progressive delays: 1s, 3s, 5s, 8s\n\n        last_exception = None\n\n        for attempt in range(max_retries):\n            try:\n                # Ensure API is configured\n                self._ensure_configured()\n\n                # Create model instance\n                model = genai.GenerativeModel(\n                    model_name=resolved_name,\n                    generation_config=generation_config,\n                    system_instruction=system_prompt if system_prompt else None,\n                )\n\n                # Generate content\n                response = model.generate_content(contents)\n\n                # Extract usage information if available\n                usage = self._extract_usage(response)\n\n                return ModelResponse(\n                    content=response.text,\n                    usage=usage,\n                    model_name=resolved_name,\n                    friendly_name="Gemini",\n                    provider=ProviderType.GOOGLE,\n                    metadata={\n                        "thinking_mode": thinking_mode if capabilities.supports_extended_thinking else None,\n                        "finish_reason": (\n                            getattr(response.candidates[0], "finish_reason", "STOP") if response.candidates else "STOP"\n                        ),\n                    },\n                )\n\n            except Exception as e:\n                last_exception = e\n\n                # Check if this is a retryable error using structured error codes\n                is_retryable = self._is_error_retryable(e)\n\n                # If this is the last attempt or not retryable, give up\n                if attempt == max_retries - 1 or not is_retryable:\n                    break\n\n                # Get progressive delay\n                delay = retry_delays[attempt]\n\n                # Log retry attempt\n                logger.warning(\n                    f"Gemini API error for model {resolved_name}, attempt {attempt + 1}/{max_retries}: {str(e)}. Retrying in {delay}s..."\n                )\n                time.sleep(delay)\n\n        # If we get here, all retries failed\n        actual_attempts = attempt + 1  # Convert from 0-based index to human-readable count\n        error_msg = f"Gemini API error for model {resolved_name} after {actual_attempts} attempt{\'s\' if actual_attempts > 1 else \'\'}: {str(last_exception)}"\n        raise RuntimeError(error_msg) from last_exception\n\n    def count_tokens(self, text: str, model_name: str) -> int:\n        """Count tokens for the given text using Gemini\'s tokenizer."""\n        self._resolve_model_name(model_name)\n\n        # For now, use a simple estimation\n        # TODO: Use actual Gemini tokenizer when available in SDK\n        # Rough estimation: ~4 characters per token for English text\n        return len(text) // 4\n\n    def get_provider_type(self) -> ProviderType:\n        """Get the provider type."""\n        return ProviderType.GOOGLE\n\n    def validate_model_name(self, model_name: str) -> bool:\n        """Validate if the model name is supported and allowed."""\n        resolved_name = self._resolve_model_name(model_name)\n\n        # First check if model is supported\n        if resolved_name not in self.SUPPORTED_MODELS:\n            return False\n\n        # Then check if model is allowed by restrictions\n        from utils.model_restrictions import get_restriction_service\n\n        restriction_service = get_restriction_service()\n        # IMPORTANT: Parameter order is (provider_type, model_name, original_name)\n        # resolved_name is the canonical model name, model_name is the user input\n        if not restriction_service.is_allowed(ProviderType.GOOGLE, resolved_name, model_name):\n            logger.debug(f"Gemini model \'{model_name}\' -> \'{resolved_name}\' blocked by restrictions")\n            return False\n\n        return True\n\n    def supports_thinking_mode(self, model_name: str) -> bool:\n        """Check if the model supports extended thinking mode."""\n        capabilities = self.get_capabilities(model_name)\n        return capabilities.supports_extended_thinking\n\n    def get_thinking_budget(self, model_name: str, thinking_mode: str) -> int:\n        """Get actual thinking token budget for a model and thinking mode."""\n        resolved_name = self._resolve_model_name(model_name)\n        model_config = self.SUPPORTED_MODELS.get(resolved_name)\n\n        if not model_config or not model_config.supports_extended_thinking:\n            return 0\n\n        if thinking_mode not in self.THINKING_BUDGETS:\n            return 0\n\n        max_thinking_tokens = model_config.max_thinking_tokens\n        if max_thinking_tokens == 0:\n            return 0\n\n        return int(max_thinking_tokens * self.THINKING_BUDGETS[thinking_mode])\n\n    def _extract_usage(self, response) -> dict[str, int]:\n        """Extract token usage from Gemini response."""\n        usage = {}\n\n        # Try to extract usage metadata from response\n        # Note: The actual structure depends on the SDK version and response format\n        if hasattr(response, "usage_metadata"):\n            metadata = response.usage_metadata\n\n            # Extract token counts with explicit None checks\n            input_tokens = None\n            output_tokens = None\n\n            if hasattr(metadata, "prompt_token_count"):\n                value = metadata.prompt_token_count\n                if value is not None:\n                    input_tokens = value\n                    usage["input_tokens"] = value\n\n            if hasattr(metadata, "candidates_token_count"):\n                value = metadata.candidates_token_count\n                if value is not None:\n                    output_tokens = value\n                    usage["output_tokens"] = value\n\n            # Calculate total only if both values are available and valid\n            if input_tokens is not None and output_tokens is not None:\n                usage["total_tokens"] = input_tokens + output_tokens\n\n        return usage\n\n    def _supports_vision(self, model_name: str) -> bool:\n        """Check if the model supports vision (image processing)."""\n        # Gemini 2.5 models support vision\n        vision_models = {\n            "gemini-2.5-flash",\n            "gemini-2.5-pro",\n            "gemini-2.0-flash",\n            "gemini-1.5-pro",\n            "gemini-1.5-flash",\n        }\n        return model_name in vision_models\n\n    def _is_error_retryable(self, error: Exception) -> bool:\n        """Determine if an error should be retried based on structured error codes.\n\n        Uses Gemini API error structure instead of text pattern matching for reliability.\n\n        Args:\n            error: Exception from Gemini API call\n\n        Returns:\n            True if error should be retried, False otherwise\n        """\n        error_str = str(error).lower()\n\n        # Check for 429 errors first - these need special handling\n        if "429" in error_str or "quota" in error_str or "resource_exhausted" in error_str:\n            # For Gemini, check for specific non-retryable error indicators\n            # These typically indicate permanent failures or quota/size limits\n            non_retryable_indicators = [\n                "quota exceeded",\n                "resource exhausted",\n                "context length",\n                "token limit",\n                "request too large",\n                "invalid request",\n                "quota_exceeded",\n                "resource_exhausted",\n            ]\n\n            # Also check if this is a structured error from Gemini SDK\n            try:\n                # Try to access error details if available\n                if hasattr(error, "details") or hasattr(error, "reason"):\n                    # Gemini API errors may have structured details\n                    error_details = getattr(error, "details", "") or getattr(error, "reason", "")\n                    error_details_str = str(error_details).lower()\n\n                    # Check for non-retryable error codes/reasons\n                    if any(indicator in error_details_str for indicator in non_retryable_indicators):\n                        logger.debug(f"Non-retryable Gemini error: {error_details}")\n                        return False\n            except Exception:\n                pass\n\n            # Check main error string for non-retryable patterns\n            if any(indicator in error_str for indicator in non_retryable_indicators):\n                logger.debug(f"Non-retryable Gemini error based on message: {error_str[:200]}...")\n                return False\n\n            # If it\'s a 429/quota error but doesn\'t match non-retryable patterns, it might be retryable rate limiting\n            logger.debug(f"Retryable Gemini rate limiting error: {error_str[:100]}...")\n            return True\n\n        # For non-429 errors, check if they\'re retryable\n        retryable_indicators = [\n            "timeout",\n            "connection",\n            "network",\n            "temporary",\n            "unavailable",\n            "retry",\n            "internal error",\n            "408",  # Request timeout\n            "500",  # Internal server error\n            "502",  # Bad gateway\n            "503",  # Service unavailable\n            "504",  # Gateway timeout\n            "ssl",  # SSL errors\n            "handshake",  # Handshake failures\n        ]\n\n        return any(indicator in error_str for indicator in retryable_indicators)\n\n    def _process_image(self, image_path: str) -> Optional[dict]:\n        """Process an image for Gemini API."""\n        try:\n            if image_path.startswith("data:image/"):\n                # Handle data URL: data:image/png;base64,iVBORw0...\n                header, data = image_path.split(",", 1)\n                mime_type = header.split(";")[0].split(":")[1]\n                return {"inline_data": {"mime_type": mime_type, "data": data}}\n            else:\n                # Handle file path\n                from utils.file_types import get_image_mime_type\n\n                if not os.path.exists(image_path):\n                    logger.warning(f"Image file not found: {image_path}")\n                    return None\n\n                # Detect MIME type from file extension using centralized mappings\n                ext = os.path.splitext(image_path)[1].lower()\n                mime_type = get_image_mime_type(ext)\n\n                # Read and encode the image\n                with open(image_path, "rb") as f:\n                    image_data = base64.b64encode(f.read()).decode()\n\n                return {"inline_data": {"mime_type": mime_type, "data": image_data}}\n        except Exception as e:\n            logger.error(f"Error processing image {image_path}: {e}")\n            return None\n')
============================== 6 failed in 0.18s ===============================
